# mix.py - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏—è
import math
import os
import logging
import random
import numpy as np
from pydub import AudioSegment, effects
from pydub.generators import Sine, WhiteNoise
import librosa
import soundfile as sf
from config import Config

class AdvancedMixer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self, sample_picker):
        self.picker = sample_picker
        self.config = Config()
        
    def create_professional_mix(self, track_data, output_dir="output"):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –º–∏–∫—Å–∞ —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏"""
        os.makedirs(output_dir, exist_ok=True)
        
        tempo = track_data.get("tempo", 120)
        structure = track_data.get("structure", [])
        tracks = track_data.get("tracks", [])
        genre = track_data.get("genre", "electronic")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ timeline
        total_duration_sec = sum(s["duration"] for s in structure)
        total_duration_ms = int(total_duration_sec * 1000)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—Ç–µ—Ä-—à–∏–Ω—ã
        master_mix = AudioSegment.silent(duration=total_duration_ms)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä—É–ø–ø–æ–≤—ã—Ö —à–∏–Ω
        drum_bus = AudioSegment.silent(duration=total_duration_ms)
        bass_bus = AudioSegment.silent(duration=total_duration_ms)
        melody_bus = AudioSegment.silent(duration=total_duration_ms)
        fx_bus = AudioSegment.silent(duration=total_duration_ms)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–µ–∫–∞
        processed_tracks = []
        
        for track_idx, track in enumerate(tracks):
            track_result = self.process_single_track(
                track, tempo, total_duration_ms, genre, output_dir
            )
            
            if track_result:
                processed_tracks.append(track_result)
                
                # –†–æ—É—Ç–∏–Ω–≥ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —à–∏–Ω—É
                track_audio = track_result["audio"]
                track_type = self.classify_track_type(track["sample_tags"])
                
                if track_type == "drums":
                    drum_bus = drum_bus.overlay(track_audio)
                elif track_type == "bass":
                    bass_bus = bass_bus.overlay(track_audio)
                elif track_type == "melody":
                    melody_bus = melody_bus.overlay(track_audio)
                else:
                    fx_bus = fx_bus.overlay(track_audio)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø–æ–≤—ã—Ö —à–∏–Ω
        drum_bus = self.process_drum_bus(drum_bus, genre)
        bass_bus = self.process_bass_bus(bass_bus, genre)
        melody_bus = self.process_melody_bus(melody_bus, genre)
        fx_bus = self.process_fx_bus(fx_bus, genre)
        
        # –°–≤–µ–¥–µ–Ω–∏–µ –≤ –º–∞—Å—Ç–µ—Ä
        master_mix = master_mix.overlay(drum_bus)
        master_mix = master_mix.overlay(bass_bus)
        master_mix = master_mix.overlay(melody_bus)
        master_mix = master_mix.overlay(fx_bus)
        
        # –ú–∞—Å—Ç–µ—Ä–∏–Ω–≥
        master_mix = self.master_processing(master_mix, genre)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        final_path = os.path.join(output_dir, "final_professional_mix.wav")
        master_mix.export(final_path, format="wav")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–µ–º–æ–≤
        self.export_stems(
            {"drums": drum_bus, "bass": bass_bus, "melody": melody_bus, "fx": fx_bus},
            output_dir
        )
        
        logging.info(f"üéõÔ∏è –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –º–∏–∫—Å –≥–æ—Ç–æ–≤: {final_path}")
        return master_mix, final_path

    def process_single_track(self, track, tempo, total_duration_ms, genre, output_dir):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–∫–∞"""
        track_name = track.get("name", "unknown")
        sample_tags = track.get("sample_tags", [])
        volume = track.get("volume", -6)
        
        # –¢–∞–π–º–∏–Ω–≥
        starts_at_beats = track.get("starts_at", 0)
        ends_at_beats = track.get("ends_at", None)
        
        beat_duration = 60.0 / tempo
        starts_at_ms = int(starts_at_beats * beat_duration * 1000)
        ends_at_ms = int(ends_at_beats * beat_duration * 1000) if ends_at_beats else total_duration_ms
        
        track_duration_ms = ends_at_ms - starts_at_ms
        
        # –ü–æ–¥–±–æ—Ä —Å—ç–º–ø–ª–æ–≤
        picked_samples = self.picker.pick_samples_enhanced(
            required_tags=sample_tags,
            target_tempo=tempo,
            genre_hint=genre,
            top_k=3
        )
        
        if not picked_samples:
            logging.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å—ç–º–ø–ª—ã –¥–ª—è {track_name}")
            return None
        
        # –í—ã–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ç–º–ø–ª–∞
        chosen_sample = picked_samples[0]
        sample_path = chosen_sample["path"]
        
        try:
            sample_audio = AudioSegment.from_file(sample_path)
            
            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            processed_audio = self.smart_duration_fitting(
                sample_audio, track_duration_ms, chosen_sample.get("category", "oneshot")
            )
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ volume
            processed_audio = processed_audio + volume
            
            # –ü–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            positioned_audio = AudioSegment.silent(duration=total_duration_ms)
            positioned_audio = positioned_audio.overlay(processed_audio, position=starts_at_ms)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ stem
            stem_path = os.path.join(output_dir, "stems", f"{track_name}.wav")
            os.makedirs(os.path.dirname(stem_path), exist_ok=True)
            positioned_audio.export(stem_path, format="wav")
            
            logging.info(f"‚úÖ [{track_name}] –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {chosen_sample['filename']}")
            
            return {
                "name": track_name,
                "audio": positioned_audio,
                "sample_info": chosen_sample,
                "stem_path": stem_path
            }
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {track_name}: {e}")
            return None

    def smart_duration_fitting(self, audio, target_duration_ms, category):
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        current_duration = len(audio)
        
        if category == "loop":
            # –î–ª—è –ª—É–ø–æ–≤ - –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ –∏–ª–∏ –æ–±—Ä–µ–∑–∫–∞
            if current_duration < target_duration_ms:
                repeats = (target_duration_ms // current_duration) + 1
                audio = (audio * repeats)[:target_duration_ms]
            else:
                audio = audio[:target_duration_ms]
        else:
            # –î–ª—è one-shots - —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ + —Ç–∏—à–∏–Ω–∞ –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ
            if current_duration < target_duration_ms:
                # –ï—Å–ª–∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π - –º–æ–∂–µ–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
                if current_duration < 2000:  # –º–µ–Ω—å—à–µ 2 —Å–µ–∫—É–Ω–¥
                    repeats = min(target_duration_ms // current_duration, 4)
                    audio = audio * repeats
                
                # –î–æ–ø–æ–ª–Ω—è–µ–º —Ç–∏—à–∏–Ω–æ–π –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
                remaining = target_duration_ms - len(audio)
                if remaining > 0:
                    audio += AudioSegment.silent(duration=remaining)
            else:
                # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π - –±–µ—Ä—ë–º –Ω–∞—á–∞–ª–æ
                audio = audio[:target_duration_ms]
        
        return audio

    def classify_track_type(self, tags):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç—Ä–µ–∫–∞ –¥–ª—è —Ä–æ—É—Ç–∏–Ω–≥–∞ –≤ —à–∏–Ω—ã"""
        drums_tags = ["kick", "snare", "hihat", "clap", "percussion", "drums"]
        bass_tags = ["bass", "808", "sub", "reese"]
        melody_tags = ["lead", "melody", "synth", "piano", "guitar", "vocal"]
        fx_tags = ["fx", "effect", "sweep", "riser", "impact"]
        
        if any(tag in tags for tag in drums_tags):
            return "drums"
        elif any(tag in tags for tag in bass_tags):
            return "bass"
        elif any(tag in tags for tag in melody_tags):
            return "melody"
        else:
            return "fx"

    def process_drum_bus(self, drum_audio, genre):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ä–∞–±–∞–Ω–Ω–æ–π —à–∏–Ω—ã"""
        if len(drum_audio.get_array_of_samples()) == 0:
            return drum_audio
            
        # –ö–æ–º–ø—Ä–µ—Å—Å–∏—è –¥–ª—è punch
        drum_audio = self.apply_compression(drum_audio, ratio=4, threshold=-12)
        
        # EQ –¥–ª—è —É–¥–∞—Ä–Ω—ã—Ö
        if genre in ["trap", "hip-hop"]:
            # Boost –Ω–∏–∑—ã –∏ –≤–µ—Ä—Ö–∞
            drum_audio = drum_audio.low_pass_filter(12000).high_pass_filter(40)
        elif genre == "techno":
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            drum_audio = drum_audio.high_pass_filter(60)
        
        return drum_audio

    def process_bass_bus(self, bass_audio, genre):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Å–æ–≤–æ–π —à–∏–Ω—ã"""
        if len(bass_audio.get_array_of_samples()) == 0:
            return bass_audio
            
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–∏—Ö —á–∞—Å—Ç–æ—Ç
        bass_audio = bass_audio.low_pass_filter(250)
        
        # –ö–æ–º–ø—Ä–µ—Å—Å–∏—è –¥–ª—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
        bass_audio = self.apply_compression(bass_audio, ratio=6, threshold=-18)
        
        # –°–∞—Ç—É—Ä–∞—Ü–∏—è –¥–ª—è –∂–∞–Ω—Ä–æ–≤
        if genre in ["trap", "dubstep"]:
            bass_audio = self.apply_saturation(bass_audio, 0.3)
        
        return bass_audio

    def process_melody_bus(self, melody_audio, genre):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ–ª–æ–¥–∏—á–µ—Å–∫–æ–π —à–∏–Ω—ã"""
        if len(melody_audio.get_array_of_samples()) == 0:
            return melody_audio
            
        # –ú—è–≥–∫–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è
        melody_audio = self.apply_compression(melody_audio, ratio=2.5, threshold=-16)
        
        # –†–µ–≤–µ—Ä–± –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
        melody_audio = self.apply_reverb(melody_audio, room_size=0.5, damping=0.3)
        
        # EQ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∂–∞–Ω—Ä–∞
        if genre == "ambient":
            melody_audio = melody_audio.high_pass_filter(200)
        elif genre in ["trap", "hip-hop"]:
            melody_audio = melody_audio.low_pass_filter(8000)
        
        return melody_audio

    def process_fx_bus(self, fx_audio, genre):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —à–∏–Ω—ã —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
        if len(fx_audio.get_array_of_samples()) == 0:
            return fx_audio
            
        # –°—Ç–µ—Ä–µ–æ—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        fx_audio = self.apply_stereo_widening(fx_audio, 1.5)
        
        # –†–µ–≤–µ—Ä–± –¥–ª—è –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã
        fx_audio = self.apply_reverb(fx_audio, room_size=0.8, damping=0.5)
        
        return fx_audio

    def master_processing(self, audio, genre):
        """–ú–∞—Å—Ç–µ—Ä–∏–Ω–≥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –º–∏–∫—Å–∞"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        audio = effects.normalize(audio)
        
        # –ú—É–ª—å—Ç–∏–±—ç–Ω–¥ –∫–æ–º–ø—Ä–µ—Å—Å–∏—è (—ç–º—É–ª—è—Ü–∏—è)
        low_band = audio.low_pass_filter(250)
        mid_band = audio.high_pass_filter(250).low_pass_filter(4000)
        high_band = audio.high_pass_filter(4000)
        
        low_band = self.apply_compression(low_band, ratio=3, threshold=-15)
        mid_band = self.apply_compression(mid_band, ratio=2, threshold=-12)
        high_band = self.apply_compression(high_band, ratio=2.5, threshold=-10)
        
        # –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ
        audio = low_band.overlay(mid_band).overlay(high_band)
        
        # –õ–∏–º–∏—Ç–∏–Ω–≥
        audio = self.apply_limiter(audio, threshold=-1)
        
        # –ñ–∞–Ω—Ä–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        if genre == "trap":
            audio = audio + 2  # –ì—Ä–æ–º—á–µ –¥–ª—è trap
        elif genre == "ambient":
            audio = audio - 3  # –¢–∏—à–µ –¥–ª—è ambient
        
        return audio

    def apply_compression(self, audio, ratio=4, threshold=-12, attack=10, release=100):
        """–≠–º—É–ª—è—Ü–∏—è –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32)
        
        if len(samples) == 0:
            return audio
            
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if audio.channels == 2:
            samples = samples.reshape((-1, 2))
        
        # –î–µ—Ç–µ–∫—Ü–∏—è —É—Ä–æ–≤–Ω—è
        if samples.ndim == 2:
            level = np.sqrt(np.mean(samples**2, axis=1))
        else:
            level = np.abs(samples)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ —Ç–∞–º, –≥–¥–µ —É—Ä–æ–≤–µ–Ω—å –ø—Ä–µ–≤—ã—à–∞–µ—Ç threshold
        threshold_linear = 10**(threshold/20)
        
        compression_mask = level > threshold_linear
        if np.any(compression_mask):
            if samples.ndim == 2:
                reduction = ((level[compression_mask] / threshold_linear) ** (1/ratio - 1))
                samples[compression_mask] *= reduction[:, np.newaxis]
            else:
                reduction = ((level[compression_mask] / threshold_linear) ** (1/ratio - 1))
                samples[compression_mask] *= reduction
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ
        samples = np.clip(samples, -1, 1)
        samples = (samples * 32767).astype(np.int16)
        
        return audio._spawn(samples.tobytes())

    def apply_saturation(self, audio, amount=0.2):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–∞—Ç—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ç–µ–ø–ª–æ—Ç—ã"""
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32) / 32768.0
        
        # –ú—è–≥–∫–∞—è —Å–∞—Ç—É—Ä–∞—Ü–∏—è
        saturated = np.tanh(samples * (1 + amount))
        saturated = np.clip(saturated * 32767, -32767, 32767).astype(np.int16)
        
        return audio._spawn(saturated.tobytes())

    def apply_reverb(self, audio, room_size=0.5, damping=0.3, wet_level=0.2):
        """–≠–º—É–ª—è—Ü–∏—è —Ä–µ–≤–µ—Ä–±–∞ —á–µ—Ä–µ–∑ delay –∏ filtering"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–º—É–ª—è—Ü–∏—è —Ä–µ–≤–µ—Ä–±–∞
        delay_times = [50, 89, 134, 187, 267]  # –º—Å
        
        reverb_audio = audio
        
        for delay_ms in delay_times:
            delay_samples = int((delay_ms / 1000) * audio.frame_rate)
            if delay_samples > 0:
                delayed = AudioSegment.silent(duration=delay_ms) + audio
                delayed = delayed.low_pass_filter(int(8000 * (1 - damping)))
                delayed = delayed - (20 + delay_ms/10)  # –ó–∞—Ç—É—Ö–∞–Ω–∏–µ
                
                # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã
                delayed = delayed[:len(reverb_audio)]
                reverb_audio = reverb_audio.overlay(delayed)
        
        # –ú–∏–∫—Å dry/wet
        dry_level = 1 - wet_level
        dry = audio.apply_gain(20 * math.log10(dry_level)) if dry_level > 0 else AudioSegment.silent(duration=len(audio))
        wet = reverb_audio.apply_gain(20 * math.log10(wet_level)) if wet_level > 0 else AudioSegment.silent(duration=len(reverb_audio))
        return dry.overlay(wet)

    def apply_stereo_widening(self, audio, width=1.5):
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å—Ç–µ—Ä–µ–æ–±–∞–∑—ã"""
        if audio.channels != 2:
            return audio
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32)
        samples = samples.reshape((-1, 2))
        
        # M/S –æ–±—Ä–∞–±–æ—Ç–∫–∞
        mid = (samples[:, 0] + samples[:, 1]) / 2
        side = (samples[:, 0] - samples[:, 1]) / 2
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ side
        side *= width
        
        # –û–±—Ä–∞—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
        left = mid + side
        right = mid - side
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∏–ø–ø–∏–Ω–≥
        stereo = np.column_stack([left, right])
        stereo = np.clip(stereo, -1, 1)
        stereo = (stereo * 32767).astype(np.int16)
        
        return audio._spawn(stereo.tobytes())

    def apply_limiter(self, audio, threshold=-1, release=50):
        """–õ–∏–º–∏—Ç–µ—Ä –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–ª–∏–ø–ø–∏–Ω–≥–∞"""
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32) / 32768.0
        
        threshold_linear = 10**(threshold/20)
        
        # –ü—Ä–æ—Å—Ç–æ–π hard limiter
        samples = np.clip(samples, -threshold_linear, threshold_linear)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ
        samples = (samples * 32767).astype(np.int16)
        return audio._spawn(samples.tobytes())

    def export_stems(self, stem_dict, output_dir):
        """–≠–∫—Å–ø–æ—Ä—Ç —Å—Ç–µ–º–æ–≤ (–≥—Ä—É–ø–ø–æ–≤—ã—Ö —à–∏–Ω)"""
        stems_dir = os.path.join(output_dir, "stems")
        os.makedirs(stems_dir, exist_ok=True)
        
        for stem_name, stem_audio in stem_dict.items():
            if len(stem_audio.get_array_of_samples()) > 0:
                stem_path = os.path.join(stems_dir, f"{stem_name}_bus.wav")
                stem_audio.export(stem_path, format="wav")
                logging.info(f"üíæ –°—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {stem_path}")

class AutomationEngine:
    """–î–≤–∏–∂–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    
    @staticmethod
    def create_volume_automation(audio, automation_points):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≥—Ä–æ–º–∫–æ—Å—Ç–∏"""
        # automation_points: [(time_ms, volume_db), ...]
        
        if len(automation_points) < 2:
            return audio
        
        segments = []
        duration = len(audio)
        
        for i in range(len(automation_points) - 1):
            start_time, start_vol = automation_points[i]
            end_time, end_vol = automation_points[i + 1]
            
            if start_time >= duration:
                break
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç
            segment_start = max(0, start_time)
            segment_end = min(duration, end_time)
            segment = audio[segment_start:segment_end]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥—Ä–∞–¥—É–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
            if len(segment) > 0:
                segment = AutomationEngine.apply_volume_curve(segment, start_vol, end_vol)
                segments.append(segment)
        
        # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        if segments:
            return sum(segments)
        return audio

    @staticmethod
    def apply_volume_curve(audio, start_db, end_db):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–ª–∞–≤–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏"""
        length = len(audio)
        if length == 0:
            return audio
            
        # –°–æ–∑–¥–∞—ë–º –∫—Ä–∏–≤—É—é –∏–∑–º–µ–Ω–µ–Ω–∏—è
        volume_curve = np.linspace(start_db, end_db, length)
        
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫—Ä–∏–≤—É—é
        for i, vol_db in enumerate(volume_curve):
            multiplier = 10**(vol_db/20)
            if audio.channels == 2:
                if i*2 + 1 < len(samples):
                    samples[i*2] *= multiplier
                    samples[i*2 + 1] *= multiplier
            else:
                if i < len(samples):
                    samples[i] *= multiplier
        
        samples = np.clip(samples, -32767, 32767).astype(np.int16)
        return audio._spawn(samples.tobytes())

class ArrangementEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞—Ä–∞–Ω–∂–∏—Ä–æ–≤–æ–∫"""
    
    def __init__(self, config):
        self.config = config
    
    def create_dynamic_arrangement(self, base_tracks, genre, mood, total_duration):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞—Ä–∞–Ω–∂–∏—Ä–æ–≤–∫–∏"""
        arranged_tracks = []
        beat_duration = 60.0 / base_tracks[0].get("tempo", 120)
        total_beats = int(total_duration / beat_duration)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–π —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é
        sections = self.generate_intensity_sections(genre, total_beats)
        
        for track in base_tracks:
            arranged_track = track.copy()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–∫–∞
            track_pattern = self.get_track_pattern(track["sample_tags"], genre, sections)
            
            arranged_track.update(track_pattern)
            arranged_tracks.append(arranged_track)
        
        return arranged_tracks

    def generate_intensity_sections(self, genre, total_beats):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π —Å —Ä–∞–∑–Ω–æ–π –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é"""
        if genre == "trap":
            return [
                {"type": "intro", "beats": (0, 32), "intensity": 0.3},
                {"type": "verse", "beats": (32, 64), "intensity": 0.6},
                {"type": "hook", "beats": (64, 96), "intensity": 1.0},
                {"type": "verse2", "beats": (96, 128), "intensity": 0.7},
                {"type": "hook2", "beats": (128, 160), "intensity": 1.0},
                {"type": "outro", "beats": (160, total_beats), "intensity": 0.4}
            ]
        elif genre == "house":
            return [
                {"type": "intro", "beats": (0, 64), "intensity": 0.4},
                {"type": "build", "beats": (64, 96), "intensity": 0.7},
                {"type": "drop", "beats": (96, 224), "intensity": 1.0},
                {"type": "breakdown", "beats": (224, 288), "intensity": 0.5},
                {"type": "drop2", "beats": (288, 416), "intensity": 1.0},
                {"type": "outro", "beats": (416, total_beats), "intensity": 0.3}
            ]
        else:
            # Generic electronic
            section_length = total_beats // 4
            return [
                {"type": "intro", "beats": (0, section_length), "intensity": 0.4},
                {"type": "build", "beats": (section_length, section_length*2), "intensity": 0.7},
                {"type": "climax", "beats": (section_length*2, section_length*3), "intensity": 1.0},
                {"type": "outro", "beats": (section_length*3, total_beats), "intensity": 0.4}
            ]

    def get_track_pattern(self, tags, genre, sections):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–∫–∞"""
        track_type = self.classify_track_role(tags)
        
        patterns = {
            "drums": {
                "intro": 0.5, "verse": 1.0, "hook": 1.0, "build": 1.0, "drop": 1.0
            },
            "bass": {
                "intro": 0.0, "verse": 0.8, "hook": 1.0, "build": 0.6, "drop": 1.0
            },
            "melody": {
                "intro": 0.3, "verse": 0.6, "hook": 1.0, "build": 0.8, "drop": 1.0
            },
            "fx": {
                "intro": 0.2, "verse": 0.3, "hook": 0.5, "build": 1.0, "drop": 0.7
            }
        }
        
        base_pattern = patterns.get(track_type, patterns["melody"])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–π–º–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–∫—Ü–∏–π
        timing = {"starts_at": 0, "automation": []}
        
        for section in sections:
            section_start_beat = section["beats"][0]
            section_intensity = section["intensity"]
            track_presence = base_pattern.get(section["type"], 0.5)
            
            final_volume = -20 + (track_presence * section_intensity * 20)
            timing["automation"].append((section_start_beat, final_volume))
        
        return timing

    def classify_track_role(self, tags):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–æ–ª–∏ —Ç—Ä–µ–∫–∞ –≤ –º–∏–∫—Å–µ"""
        if any(tag in ["kick", "snare", "hihat", "drums"] for tag in tags):
            return "drums"
        elif any(tag in ["bass", "808", "sub"] for tag in tags):
            return "bass"
        elif any(tag in ["lead", "melody", "synth", "vocal"] for tag in tags):
            return "melody"
        else:
            return "fx"

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
class ExportManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
    
    @staticmethod
    def export_to_wav(audio, path, quality="high"):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ WAV —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º"""
        if quality == "high":
            audio.export(path, format="wav", parameters=["-ar", "48000", "-ac", "2"])
        elif quality == "medium":
            audio.export(path, format="wav", parameters=["-ar", "44100", "-ac", "2"])
        else:
            audio.export(path, format="wav")
    
    @staticmethod
    def export_to_mp3(audio, path, bitrate="320k"):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤ MP3"""
        audio.export(path, format="mp3", bitrate=bitrate)
    
    @staticmethod
    def export_stems_package(stems_dict, output_dir):
        """–≠–∫—Å–ø–æ—Ä—Ç –ø–∞–∫–µ—Ç–∞ —Å—Ç–µ–º–æ–≤"""
        stems_dir = os.path.join(output_dir, "stems_package")
        os.makedirs(stems_dir, exist_ok=True)
        
        for stem_name, stem_audio in stems_dict.items():
            # WAV –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            wav_path = os.path.join(stems_dir, f"{stem_name}.wav")
            ExportManager.export_to_wav(stem_audio, wav_path, "high")
            
            # MP3 –¥–ª—è –¥–µ–º–æ
            mp3_path = os.path.join(stems_dir, f"{stem_name}_demo.mp3")
            ExportManager.export_to_mp3(stem_audio, mp3_path, "192k")

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏
class ExternalIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏ –∏ API"""
    
    @staticmethod
    def upload_to_soundcloud(file_path, title, description="", tags=None):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ SoundCloud"""
        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å SoundCloud API
        logging.info(f"üéµ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ SoundCloud: {title}")
        return f"https://soundcloud.com/user/track-{hash(title)}"
    
    @staticmethod  
    def analyze_with_ai(audio_path):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–∫–∞"""
        # –ó–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–µ—Ä–≤–∏—Å–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –º—É–∑—ã–∫–∏
        return {
            "genre_confidence": 0.85,
            "energy_level": 0.7,
            "danceability": 0.8,
            "valence": 0.6
        }

# –°–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
    
    def __init__(self, cache_dir="cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
    def get_audio_cache_key(self, file_path, operation="analysis"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞"""
        import hashlib
        stat = os.stat(file_path)
        content = f"{file_path}_{stat.st_size}_{stat.st_mtime}_{operation}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def cache_analysis_result(self, file_path, result):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        cache_key = self.get_audio_cache_key(file_path)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        try:
            import json
            with open(cache_file, 'w') as f:
                json.dump(result, f)
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def get_cached_analysis(self, file_path):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        cache_key = self.get_audio_cache_key(file_path)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        
        if os.path.exists(cache_file):
            try:
                import json
                with open(cache_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logging.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
        
        return None

# –°–∏—Å—Ç–µ–º–∞ –ø–ª–∞–≥–∏–Ω–æ–≤ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
class PluginManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø–ª–∞–≥–∏–Ω–æ–≤ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.plugins = {}
        self.load_builtin_plugins()
    
    def load_builtin_plugins(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø–ª–∞–≥–∏–Ω–æ–≤"""
        self.register_plugin("tempo_detection", self.advanced_tempo_detection)
        self.register_plugin("genre_classification", self.ai_genre_classifier)
        self.register_plugin("mood_analysis", self.mood_analyzer)
    
    def register_plugin(self, name, function):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        self.plugins[name] = function
        logging.info(f"üîå –ü–ª–∞–≥–∏–Ω –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω: {name}")
    
    def run_plugin(self, name, *args, **kwargs):
        """–ó–∞–ø—É—Å–∫ –ø–ª–∞–≥–∏–Ω–∞"""
        if name in self.plugins:
            return self.plugins[name](*args, **kwargs)
        else:
            logging.warning(f"‚ö†Ô∏è –ü–ª–∞–≥–∏–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω: {name}")
            return None
    
    def advanced_tempo_detection(self, audio_path):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–ø–∞"""
        try:
            y, sr = librosa.load(audio_path, duration=30)
            
            # –ù–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            tempo1, _ = librosa.beat.beat_track(y=y, sr=sr)
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ onset detection
            onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
            if len(onset_frames) > 1:
                onset_times = librosa.frames_to_time(onset_frames, sr=sr)
                intervals = np.diff(onset_times)
                if len(intervals) > 0:
                    avg_interval = np.median(intervals)
                    tempo2 = 60.0 / avg_interval
                else:
                    tempo2 = tempo1
            else:
                tempo2 = tempo1
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_tempo = np.mean([tempo1, tempo2])
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
            if final_tempo < 70:
                final_tempo *= 2
            elif final_tempo > 200:
                final_tempo /= 2
                
            return {"tempo": float(final_tempo), "confidence": 0.8}
            
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º–ø–∞: {e}")
            return {"tempo": 120, "confidence": 0.1}
    
    def ai_genre_classifier(self, audio_path):
        """AI –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∂–∞–Ω—Ä–∞ (–∑–∞–≥–ª—É—à–∫–∞)"""
        # –ó–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ML –º–æ–¥–µ–ª—å—é
        return {
            "genre": "electronic",
            "confidence": 0.5,
            "sub_genres": ["ambient", "techno", "house"]
        }
    
    def mood_analyzer(self, audio_path):
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç—Ä–µ–∫–∞"""
        try:
            y, sr = librosa.load(audio_path, duration=30)
            
            # –ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            energy = librosa.feature.rms(y=y).mean()
            
            # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
            mood_scores = {}
            
            # –≠–Ω–µ—Ä–≥–∏—á–Ω–æ—Å—Ç—å
            if energy > 0.1 and tempo > 120:
                mood_scores["energetic"] = 0.8
            
            # –¢–µ–º–Ω–æ—Ç–∞ (–Ω–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã + –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ç–µ–º–ø)
            if spectral_centroid < 2000 and tempo < 100:
                mood_scores["dark"] = 0.7
            
            # –ú–µ–ª–æ–¥–∏—á–Ω–æ—Å—Ç—å (—Å—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã + —É–º–µ—Ä–µ–Ω–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è)
            if 2000 < spectral_centroid < 6000 and 0.05 < energy < 0.15:
                mood_scores["melodic"] = 0.6
            
            # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å (–≤—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è + –≤—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã)
            if energy > 0.15 and spectral_centroid > 4000:
                mood_scores["aggressive"] = 0.8
            
            return mood_scores
            
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è: {e}")
            return {"neutral": 0.5}