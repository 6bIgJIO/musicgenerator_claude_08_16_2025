# –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô musicgen_wrapper.py —Å —Ä–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∞—É–¥–∏–æ

import torch
import torchaudio
import io
import numpy as np
import soundfile as sf
from typing import Optional, List, Dict, Union
import warnings
import logging
import time
from pathlib import Path

warnings.filterwarnings("ignore")

try:
    from audiocraft.models import musicgen
    from audiocraft.models.musicgen import MusicGen
    MUSICGEN_AVAILABLE = True
except ImportError:
    MUSICGEN_AVAILABLE = False
    logging.warning("AudioCraft not available - using fallback generation")

class MusicGenEngine:
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –¥–≤–∏–∂–æ–∫ MusicGen —Å —Ä–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∞—É–¥–∏–æ
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
    - –£–±—Ä–∞–Ω—ã –∑–∞–≥–ª—É—à–∫–∏ —Å —Ç–∏—à–∏–Ω–æ–π
    - –†–µ–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å MusicGen –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    - –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π fallback –µ—Å–ª–∏ MusicGen –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∞—É–¥–∏–æ
    """
    
    def __init__(self, model_name: str = "facebook/musicgen-medium"):
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.MUSICGEN_AVAILABLE = False
        self._load_model()

    def _load_model(self):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ MusicGen —Å fallback"""
        try:
            if not MUSICGEN_AVAILABLE:
                self.logger.warning("‚ùå AudioCraft –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
                return
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            fallback_models = [
                self.model_name,
                "facebook/musicgen-small",
                "facebook/musicgen-medium", 
                "facebook/musicgen-large",
                "D:/2027/audiocraft/audiocraft/models/facebook/musicgen-medium"
            ]
            
            for name in fallback_models:
                try:
                    self.logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {name}")
                    
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    self.model = musicgen.MusicGen.get_pretrained(name, device=self.device)
                    self.model_name = name
                    
                    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
                    self.model.set_generation_params(duration=5, use_sampling=True)
                    
                    self.logger.info(f"‚úÖ MusicGen —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {name} –Ω–∞ {self.device}")
                    self.MUSICGEN_AVAILABLE = True
                    return
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {name}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
            self.logger.warning("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å MusicGen")
            self.MUSICGEN_AVAILABLE = False
            
        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MusicGen: {e}")
            self.MUSICGEN_AVAILABLE = False

    async def generate(
        self,
        prompt: str,
        duration: int = 30,
        temperature: float = 1.0,
        top_k: int = 250,
        top_p: float = 0.0,
        genre_hint: Optional[str] = None
    ) -> bytes:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ MusicGen –∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π fallback
        
        Args:
            prompt: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            duration: –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            top_k: Top-K —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ  
            top_p: Top-P —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            genre_hint: –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø–æ –∂–∞–Ω—Ä—É
            
        Returns:
            bytes: WAV –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ
        """
        
        self.logger.info(f"üéº –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—É–¥–∏–æ: '{prompt}' ({duration}s)")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ MusicGen
        if self.MUSICGEN_AVAILABLE and self.model:
            try:
                return await self._generate_with_musicgen(
                    prompt, duration, temperature, top_k, top_p, genre_hint
                )
            except Exception as e:
                self.logger.error(f"‚ùå MusicGen –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                self.logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π fallback")
        
        # Fallback - –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        return await self._generate_high_quality_fallback(prompt, duration, genre_hint)

    async def _generate_with_musicgen(
        self, prompt: str, duration: int, temperature: float, 
        top_k: int, top_p: float, genre_hint: Optional[str]
    ) -> bytes:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–π MusicGen"""
        
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            safe_duration = min(duration, 30)  # MusicGen —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ç—Ä–µ–∫–∞–º–∏
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.model.set_generation_params(
                duration=safe_duration,
                use_sampling=True,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p
            )

            # –£–ª—É—á—à–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è MusicGen
            enhanced_prompt = self._enhance_prompt_for_genre(prompt, genre_hint)
            self.logger.info(f"üìù Enhanced prompt: {enhanced_prompt}")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞—É–¥–∏–æ
            with torch.no_grad():
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                wav_tensor = self.model.generate([enhanced_prompt])

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if wav_tensor is None or wav_tensor.size(0) == 0:
                raise RuntimeError("MusicGen –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

            self.logger.info(f"üîä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–Ω–∑–æ—Ä: {wav_tensor.shape}")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ numpy array
            if wav_tensor.dim() == 3:  # [batch, channels, samples]
                audio_array = wav_tensor[0].cpu().numpy()
            elif wav_tensor.dim() == 2:  # [channels, samples] –∏–ª–∏ [batch, samples]
                audio_array = wav_tensor.cpu().numpy()
            else:
                raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä–∞: {wav_tensor.shape}")

            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞–º–ø–ª–∏—Ç—É–¥—É –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            if audio_array.max() > 1.0 or audio_array.min() < -1.0:
                audio_array = audio_array / max(abs(audio_array.max()), abs(audio_array.min()))

            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∞—É–¥–∏–æ –Ω–µ —Ç–∏—à–∏–Ω–∞
            rms = np.sqrt(np.mean(audio_array**2))
            self.logger.info(f"üîä RMS —É—Ä–æ–≤–µ–Ω—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ: {rms:.6f}")

            if rms < 1e-6:
                raise ValueError("MusicGen —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç–∏—à–∏–Ω—É!")

            # –ï—Å–ª–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–µ —á–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ, –ø–æ–≤—Ç–æ—Ä—è–µ–º/–¥–æ–ø–æ–ª–Ω—è–µ–º
            sample_rate = self.model.sample_rate
            current_duration = audio_array.shape[-1] / sample_rate
            
            if duration > current_duration + 5:  # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ
                # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∞—É–¥–∏–æ —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
                audio_array = self._extend_audio_with_variations(
                    audio_array, sample_rate, duration
                )

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WAV bytes
            buffer = io.BytesIO()
            
            if audio_array.ndim == 1:  # –ú–æ–Ω–æ
                sf.write(buffer, audio_array, sample_rate, format='WAV')
            else:  # –°—Ç–µ—Ä–µ–æ
                if audio_array.shape[0] == 2:  # [channels, samples]
                    sf.write(buffer, audio_array.T, sample_rate, format='WAV')
                else:  # [samples, channels] –∏–ª–∏ —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ
                    sf.write(buffer, audio_array[0], sample_rate, format='WAV')

            audio_bytes = buffer.getvalue()
            buffer.close()

            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            if len(audio_bytes) < 1000:
                raise ValueError(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª: {len(audio_bytes)} bytes")

            self.logger.info(f"‚úÖ MusicGen SUCCESS: {len(audio_bytes)} bytes")
            return audio_bytes

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ _generate_with_musicgen: {e}")
            raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è fallback

    def _extend_audio_with_variations(
        self, audio_array: np.ndarray, sample_rate: int, target_duration: int
    ) -> np.ndarray:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞—É–¥–∏–æ —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        current_samples = audio_array.shape[-1]
        target_samples = int(target_duration * sample_rate)
        
        if target_samples <= current_samples:
            return audio_array[:target_samples] if audio_array.ndim == 1 else audio_array[:, :target_samples]
        
        # –°–æ–∑–¥–∞—ë–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
        extended_audio = []
        current_pos = 0
        
        while current_pos < target_samples:
            remaining_samples = target_samples - current_pos
            
            if remaining_samples >= current_samples:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—É—é –∫–æ–ø–∏—é —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
                segment = audio_array.copy()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—ë–≥–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ (pitch shift, –≤—Ä–µ–º—è, —ç—Ñ—Ñ–µ–∫—Ç—ã)
                variation_factor = 0.95 + (np.random.random() * 0.1)  # 0.95-1.05
                if audio_array.ndim == 1:
                    segment = segment * variation_factor
                else:
                    segment = segment * variation_factor
                
                extended_audio.append(segment)
                current_pos += current_samples
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω—É—é –∫–æ–ø–∏—é
                if audio_array.ndim == 1:
                    segment = audio_array[:remaining_samples]
                else:
                    segment = audio_array[:, :remaining_samples]
                extended_audio.append(segment)
                current_pos += remaining_samples
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        if audio_array.ndim == 1:
            return np.concatenate(extended_audio)
        else:
            return np.concatenate(extended_audio, axis=1)

    async def _generate_high_quality_fallback(
        self, prompt: str, duration: int, genre_hint: Optional[str]
    ) -> bytes:
        """
        –í–´–°–û–ö–û–ö–ê–ß–ï–°–¢–í–ï–ù–ù–ê–Ø fallback –≥–µ–Ω–µ—Ä–∞—Ü–∏—è - –ù–ï –¢–ò–®–ò–ù–ê!
        
        –°–æ–∑–¥–∞—ë—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –º—É–∑—ã–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏ –∂–∞–Ω—Ä–∞
        """
        
        self.logger.info(f"üéµ –°–æ–∑–¥–∞—ë–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π fallback: {genre_hint or 'auto'}")
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            music_characteristics = self._analyze_prompt(prompt, genre_hint)
            
            # –°–æ–∑–¥–∞—ë–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –∫–æ–º–ø–æ–∑–∏—Ü–∏—é
            composition = self._create_full_composition(
                duration, music_characteristics
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WAV bytes
            sample_rate = 44100
            buffer = io.BytesIO()
            sf.write(buffer, composition, sample_rate, format='WAV')
            audio_bytes = buffer.getvalue()
            buffer.close()
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ —Ç–∏—à–∏–Ω–∞
            if len(audio_bytes) < 1000:
                raise ValueError("Fallback –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–ª–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª!")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–∏–≥–Ω–∞–ª–∞
            test_rms = np.sqrt(np.mean(composition**2))
            if test_rms < 1e-6:
                raise ValueError("Fallback –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–ª–∞ —Ç–∏—à–∏–Ω—É!")
            
            self.logger.info(f"‚úÖ High-quality fallback SUCCESS: {len(audio_bytes)} bytes, RMS: {test_rms:.6f}")
            return audio_bytes
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ high-quality fallback: {e}")
            # –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π fallback
            return self._create_emergency_audio(duration)

    def _analyze_prompt(self, prompt: str, genre_hint: Optional[str]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º—É–∑—ã–∫–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        
        prompt_lower = prompt.lower()
        
        characteristics = {
            "genre": genre_hint or "electronic",
            "bpm": 120,
            "energy": 0.5,
            "mood": "neutral",
            "instruments": [],
            "style_tags": []
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º BPM
        import re
        bpm_matches = re.findall(r'(\d{2,3})\s*bpm', prompt_lower)
        if bpm_matches:
            characteristics["bpm"] = int(bpm_matches[0])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–Ω–µ—Ä–≥–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        energy_keywords = {
            "aggressive": 0.9,
            "hard": 0.8,
            "energetic": 0.8,
            "intense": 0.8,
            "dark": 0.7,
            "powerful": 0.8,
            "chill": 0.3,
            "calm": 0.2,
            "soft": 0.3,
            "peaceful": 0.2,
            "ambient": 0.2,
            "lofi": 0.3
        }
        
        for keyword, energy_level in energy_keywords.items():
            if keyword in prompt_lower:
                characteristics["energy"] = max(characteristics["energy"], energy_level)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
        if any(word in prompt_lower for word in ["dark", "aggressive", "hard", "intense"]):
            characteristics["mood"] = "dark"
        elif any(word in prompt_lower for word in ["chill", "calm", "peaceful", "soft"]):
            characteristics["mood"] = "calm"
        elif any(word in prompt_lower for word in ["happy", "upbeat", "bright", "positive"]):
            characteristics["mood"] = "happy"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        instrument_keywords = {
            "piano": "piano",
            "guitar": "guitar", 
            "bass": "bass",
            "drum": "drums",
            "violin": "violin",
            "saxophone": "saxophone",
            "synth": "synthesizer",
            "808": "808_drum"
        }
        
        for keyword, instrument in instrument_keywords.items():
            if keyword in prompt_lower:
                characteristics["instruments"].append(instrument)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∂–∞–Ω—Ä –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω
        if not genre_hint:
            genre_keywords = {
                "trap": ["trap", "hip hop", "rap", "urban"],
                "house": ["house", "edm", "dance", "club"],
                "techno": ["techno", "electronic", "industrial"],
                "ambient": ["ambient", "atmospheric", "spacious"],
                "lofi": ["lofi", "lo-fi", "jazz", "vintage"],
                "rock": ["rock", "metal", "punk"],
                "pop": ["pop", "mainstream", "commercial"]
            }
            
            for genre, keywords in genre_keywords.items():
                if any(keyword in prompt_lower for keyword in keywords):
                    characteristics["genre"] = genre
                    break
        
        return characteristics

    def _create_full_composition(self, duration: int, characteristics: Dict) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –º—É–∑—ã–∫–∞–ª—å–Ω–æ–π –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏"""
        
        sample_rate = 44100
        total_samples = int(duration * sample_rate)
        
        genre = characteristics["genre"]
        bpm = characteristics["bpm"]
        energy = characteristics["energy"]
        mood = characteristics["mood"]
        
        self.logger.info(f"  üéº –ö–æ–º–ø–æ–∑–∏—Ü–∏—è: {genre}, {bpm}BPM, —ç–Ω–µ—Ä–≥–∏—è {energy}, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ {mood}")
        
        # –°–æ–∑–¥–∞—ë–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º—É–∑—ã–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
        layers = {}
        
        # 1. –†–∏—Ç–º-—Å–µ–∫—Ü–∏—è (–æ—Å–Ω–æ–≤–∞)
        layers["rhythm"] = self._create_rhythm_section(
            total_samples, sample_rate, bpm, genre, energy
        )
        
        # 2. –ë–∞—Å–æ–≤–∞—è –ª–∏–Ω–∏—è
        layers["bass"] = self._create_bass_line(
            total_samples, sample_rate, bpm, genre, energy
        )
        
        # 3. –ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–∏–π —Å–ª–æ–π (–∞–∫–∫–æ—Ä–¥—ã/–ø–∞–¥—ã)
        layers["harmony"] = self._create_harmony_layer(
            total_samples, sample_rate, bpm, genre, mood
        )
        
        # 4. –ú–µ–ª–æ–¥–∏—è
        layers["melody"] = self._create_melody_layer(
            total_samples, sample_rate, bmp, genre, mood, energy
        )
        
        # 5. –ê—Ç–º–æ—Å—Ñ–µ—Ä–∞/—Ç–µ–∫—Å—Ç—É—Ä—ã
        layers["atmosphere"] = self._create_atmosphere_layer(
            total_samples, sample_rate, genre, mood
        )
        
        # –ú–∏–∫—à–∏—Ä—É–µ–º —Å–ª–æ–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏
        mix_levels = self._get_genre_mix_levels(genre)
        
        final_composition = np.zeros(total_samples)
        
        for layer_name, layer_audio in layers.items():
            if layer_audio is not None and len(layer_audio) > 0:
                # –ü–æ–¥–≥–æ–Ω—è–µ–º –¥–ª–∏–Ω—É
                if len(layer_audio) != total_samples:
                    if len(layer_audio) > total_samples:
                        layer_audio = layer_audio[:total_samples]
                    else:
                        # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º
                        repetitions = total_samples // len(layer_audio) + 1
                        layer_audio = np.tile(layer_audio, repetitions)[:total_samples]
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —É—Ä–æ–≤–µ–Ω—å –º–∏–∫—Å–∞
                level = mix_levels.get(layer_name, 0.5)
                final_composition += layer_audio * level
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        final_composition = self._apply_final_processing(
            final_composition, sample_rate, genre, energy
        )
        
        return final_composition

    def _create_rhythm_section(
        self, total_samples: int, sample_rate: int, bpm: int, genre: str, energy: float
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∏—Ç–º-—Å–µ–∫—Ü–∏–∏"""
        
        beat_duration = int(sample_rate * 60 / bpm)  # –°–µ–º–ø–ª—ã –Ω–∞ beat
        
        # –°–æ–∑–¥–∞—ë–º –±–∞–∑–æ–≤—ã–µ drum sounds
        kick_sound = self._create_kick_sound(sample_rate, genre, energy)
        snare_sound = self._create_snare_sound(sample_rate, genre, energy)
        hihat_sound = self._create_hihat_sound(sample_rate, genre, energy)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤
        patterns = {
            "trap": {
                "kick": [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0],
                "snare": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
                "hihat": [1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]
            },
            "house": {
                "kick": [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
                "snare": [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0],
                "hihat": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
            },
            "techno": {
                "kick": [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],
                "snare": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                "hihat": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
            }
        }
        
        pattern = patterns.get(genre, patterns["house"])  # Default to house
        
        rhythm_track = np.zeros(total_samples)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        step_duration = beat_duration // 4  # 16-step sequencer
        
        current_pos = 0
        while current_pos < total_samples:
            # Kick pattern
            for i, hit in enumerate(pattern["kick"]):
                if hit:
                    pos = current_pos + (i * step_duration)
                    end_pos = min(pos + len(kick_sound), total_samples)
                    if pos < total_samples:
                        rhythm_track[pos:end_pos] += kick_sound[:end_pos-pos]
            
            # Snare pattern
            for i, hit in enumerate(pattern["snare"]):
                if hit:
                    pos = current_pos + (i * step_duration)
                    end_pos = min(pos + len(snare_sound), total_samples)
                    if pos < total_samples:
                        rhythm_track[pos:end_pos] += snare_sound[:end_pos-pos]
            
            # Hihat pattern
            for i, hit in enumerate(pattern["hihat"]):
                if hit:
                    pos = current_pos + (i * step_duration)
                    end_pos = min(pos + len(hihat_sound), total_samples)
                    if pos < total_samples:
                        rhythm_track[pos:end_pos] += hihat_sound[:end_pos-pos]
            
            current_pos += beat_duration * 4  # One bar
        
        return rhythm_track

    def _create_kick_sound(self, sample_rate: int, genre: str, energy: float) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ kick drum sound"""
        duration = 0.5  # 500ms
        samples = int(sample_rate * duration)
        t = np.linspace(0, duration, samples)
        
        # –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∂–∞–Ω—Ä–∞
        base_freq = 60 if genre == "trap" else 50
        
        # –°–æ–∑–¥–∞—ë–º kick —Å envelope –∏ pitch sweep
        envelope = np.exp(-t * 8)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        freq_sweep = base_freq * (1 + np.exp(-t * 10))  # Frequency sweep
        
        kick = np.sin(2 * np.pi * freq_sweep * t) * envelope
        kick = kick * (0.7 + energy * 0.3)  # –£—Ä–æ–≤–µ–Ω—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —ç–Ω–µ—Ä–≥–∏–∏
        
        return kick

    def _create_snare_sound(self, sample_rate: int, genre: str, energy: float) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ snare drum sound"""
        duration = 0.2  # 200ms
        samples = int(sample_rate * duration)
        t = np.linspace(0, duration, samples)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —à—É–º –∏ —Ç–æ–Ω
        noise = np.random.normal(0, 1, samples)
        tone = np.sin(2 * np.pi * 200 * t)  # 200Hz tone
        
        envelope = np.exp(-t * 15)  # –ë—ã—Å—Ç—Ä–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        
        snare = (noise * 0.7 + tone * 0.3) * envelope
        snare = snare * (0.5 + energy * 0.5)
        
        return snare

    def _create_hihat_sound(self, sample_rate: int, genre: str, energy: float) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ hi-hat sound"""
        duration = 0.1  # 100ms
        samples = int(sample_rate * duration)
        
        # –í—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π —à—É–º
        noise = np.random.normal(0, 1, samples)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–ø—Ä–æ—Å—Ç–∞—è –∏–º–∏—Ç–∞—Ü–∏—è high-pass)
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å proper digital filter
        for i in range(1, len(noise)):
            noise[i] = noise[i] - 0.95 * noise[i-1]  # –ü—Ä–æ—Å—Ç–æ–π high-pass
        
        envelope = np.exp(-np.linspace(0, 5, samples))  # –†–µ–∑–∫–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        
        hihat = noise * envelope * (0.3 + energy * 0.2)
        
        return hihat

    def _create_bass_line(
        self, total_samples: int, sample_rate: int, bpm: int, genre: str, energy: float
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Å–æ–≤–æ–π –ª–∏–Ω–∏–∏"""
        
        bass_track = np.zeros(total_samples)
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–æ—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤
        bass_notes = {
            "trap": [55, 65.41, 73.42],  # A1, C2, D2
            "house": [41.20, 49.00, 55.00],  # E1, G1, A1
            "techno": [55, 61.74, 65.41],  # A1, B1, C2
        }
        
        notes = bass_notes.get(genre, bass_notes["house"])
        
        beat_duration = int(sample_rate * 60 / bpm)
        note_duration = beat_duration * 2  # Half notes
        
        current_pos = 0
        note_index = 0
        
        while current_pos < total_samples:
            note_freq = notes[note_index % len(notes)]
            
            # –°–æ–∑–¥–∞—ë–º –±–∞—Å–æ–≤—É—é –Ω–æ—Ç—É
            note_samples = min(note_duration, total_samples - current_pos)
            t = np.linspace(0, note_samples / sample_rate, note_samples)
            
            # –ë–∞—Å–æ–≤—ã–π –∑–≤—É–∫ —Å –≥–∞—Ä–º–æ–Ω–∏–∫–∞–º–∏
            fundamental = np.sin(2 * np.pi * note_freq * t)
            harmonic2 = np.sin(2 * np.pi * note_freq * 2 * t) * 0.3
            harmonic3 = np.sin(2 * np.pi * note_freq * 3 * t) * 0.1
            
            bass_note = fundamental + harmonic2 + harmonic3
            
            # Envelope
            envelope = np.exp(-t * 2) * (1 - np.exp(-t * 20))  # ADSR-like
            bass_note *= envelope
            
            bass_note *= (0.4 + energy * 0.3)  # Amplitude based on energy
            
            bass_track[current_pos:current_pos + note_samples] += bass_note
            
            current_pos += note_duration
            note_index += 1
        
        return bass_track

    def _create_harmony_layer(
        self, total_samples: int, sample_rate: int, bpm: int, genre: str, mood: str
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è (–∞–∫–∫–æ—Ä–¥—ã/–ø–∞–¥—ã)"""
        
        harmony_track = np.zeros(total_samples)
        
        # –í—ã–±–∏—Ä–∞–µ–º –∞–∫–∫–æ—Ä–¥—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        if mood == "dark":
            chord_frequencies = [
                [220, 261.63, 311.13],  # Am
                [196, 233.08, 277.18],  # G
                [246.94, 293.66, 349.23]  # F
            ]
        elif mood == "happy":
            chord_frequencies = [
                [261.63, 329.63, 392.00],  # C
                [293.66, 369.99, 440.00],  # D
                [329.63, 415.30, 493.88]   # E
            ]
        else:  # neutral
            chord_frequencies = [
                [220, 277.18, 329.63],  # Am
                [261.63, 329.63, 392.00],  # C
                [196, 246.94, 293.66]   # G
            ]
        
        chord_duration = int(sample_rate * 60 / bpm * 8)  # 8 beats per chord
        
        current_pos = 0
        chord_index = 0
        
        while current_pos < total_samples:
            chord_freqs = chord_frequencies[chord_index % len(chord_frequencies)]
            
            chord_samples = min(chord_duration, total_samples - current_pos)
            t = np.linspace(0, chord_samples / sample_rate, chord_samples)
            
            # –°–æ–∑–¥–∞—ë–º –∞–∫–∫–æ—Ä–¥
            chord = np.zeros(chord_samples)
            for freq in chord_freqs:
                chord += np.sin(2 * np.pi * freq * t) / len(chord_freqs)
            
            # Soft envelope –¥–ª—è –ø–∞–¥–∞
            envelope = 1 - np.exp(-t * 5)  # Fade in
            chord *= envelope * 0.3  # Quiet level for harmony
            
            harmony_track[current_pos:current_pos + chord_samples] += chord
            
            current_pos += chord_duration
            chord_index += 1
        
        return harmony_track

    def _create_melody_layer(
        self, total_samples: int, sample_rate: int, bpm: int, genre: str, mood: str, energy: float
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ–ª–æ–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ—è"""
        
        melody_track = np.zeros(total_samples)
        
        # –ú–µ–ª–æ–¥–∏—á–µ—Å–∫–∏–µ –Ω–æ—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        if mood == "dark":
            melody_notes = [220, 246.94, 261.63, 293.66, 329.63]  # A minor scale
        elif mood == "happy":
            melody_notes = [261.63, 293.66, 329.63, 349.23, 392.00]  # C major scale
        else:
            melody_notes = [220, 261.63, 293.66, 329.63, 369.99]  # Mixed
        
        note_duration = int(sample_rate * 60 / bpm)  # Quarter notes
        
        current_pos = 0
        
        while current_pos < total_samples:
            # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –Ω–æ—Ç—É –∏–∑ –≥–∞–º–º—ã
            note_freq = np.random.choice(melody_notes)
            
            # –°–ª—É—á–∞–π–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–æ—Ç—ã
            current_note_duration = np.random.choice([
                note_duration // 2,  # Eighth note
                note_duration,       # Quarter note
                note_duration * 2    # Half note
            ])
            
            current_note_duration = min(current_note_duration, total_samples - current_pos)
            
            if current_note_duration <= 0:
                break
            
            t = np.linspace(0, current_note_duration / sample_rate, current_note_duration)
            
            # –°–æ–∑–¥–∞—ë–º –º–µ–ª–æ–¥–∏—á–µ—Å–∫—É—é –Ω–æ—Ç—É
            note = np.sin(2 * np.pi * note_freq * t)
            
            # ADSR envelope
            attack = int(current_note_duration * 0.1)
            decay = int(current_note_duration * 0.2)
            sustain_level = 0.7
            release = int(current_note_duration * 0.3)
            
            envelope = np.ones(current_note_duration)
            
            # Attack
            if attack > 0:
                envelope[:attack] = np.linspace(0, 1, attack)
            
            # Decay
            if decay > 0 and attack + decay < current_note_duration:
                envelope[attack:attack+decay] = np.linspace(1, sustain_level, decay)
            
            # Release
            if release > 0:
                envelope[-release:] = np.linspace(envelope[-release], 0, release)
            
            note *= envelope * (0.4 + energy * 0.2)
            
            melody_track[current_pos:current_pos + current_note_duration] += note
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—É–∑—ã –º–µ–∂–¥—É –Ω–æ—Ç–∞–º–∏ –∏–Ω–æ–≥–¥–∞
            if np.random.random() < 0.3:
                current_pos += current_note_duration + (note_duration // 4)
            else:
                current_pos += current_note_duration
        
        return melody_track

    def _create_atmosphere_layer(
        self, total_samples: int, sample_rate: int, genre: str, mood: str
    ) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–æ–≥–æ —Å–ª–æ—è"""
        
        if genre in ["ambient", "cinematic"] or mood == "calm":
            # –°–æ–∑–¥–∞—ë–º ambient pad
            atmosphere = np.zeros(total_samples)
            
            # –ù–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–π pad
            t = np.linspace(0, total_samples / sample_rate, total_samples)
            pad_freq = 110  # A2
            
            pad = np.sin(2 * np.pi * pad_freq * t)
            pad += np.sin(2 * np.pi * pad_freq * 1.5 * t) * 0.5  # Perfect fifth
            
            # –ú–µ–¥–ª–µ–Ω–Ω–∞—è –º–æ–¥—É–ª—è—Ü–∏—è
            modulation = 1 + 0.1 * np.sin(2 * np.pi * 0.1 * t)  # 0.1 Hz LFO
            pad *= modulation
            
            # –ú—è–≥–∫–æ–µ fade in/out
            fade_samples = total_samples // 10
            fade_in = np.linspace(0, 1, fade_samples)
            fade_out = np.linspace(1, 0, fade_samples)
            
            pad[:fade_samples] *= fade_in
            pad[-fade_samples:] *= fade_out
            
            atmosphere = pad * 0.2  # Very quiet
            
            return atmosphere
        
        elif genre == "lofi":
            # –í–∏–Ω—Ç–∞–∂–Ω—ã–µ —à—É–º—ã –∏ —Ç–µ–∫—Å—Ç—É—Ä—ã
            vinyl_noise = np.random.normal(0, 0.02, total_samples)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –≤–∏–Ω—Ç–∞–∂–Ω–æ–≥–æ –∑–≤—É–∫–∞
            for i in range(1, len(vinyl_noise)):
                vinyl_noise[i] = vinyl_noise[i] * 0.7 + vinyl_noise[i-1] * 0.3
            
            return vinyl_noise
        
        else:
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞
            return np.zeros(total_samples)

    def _get_genre_mix_levels(self, genre: str) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –º–∏–∫—Å–∞ –¥–ª—è –∂–∞–Ω—Ä–∞"""
        
        mix_levels = {
            "trap": {
                "rhythm": 0.8,
                "bass": 0.6,
                "harmony": 0.3,
                "melody": 0.5,
                "atmosphere": 0.1
            },
            "house": {
                "rhythm": 0.7,
                "bass": 0.5,
                "harmony": 0.4,
                "melody": 0.6,
                "atmosphere": 0.2
            },
            "ambient": {
                "rhythm": 0.3,
                "bass": 0.4,
                "harmony": 0.8,
                "melody": 0.5,
                "atmosphere": 0.6
            },
            "techno": {
                "rhythm": 0.8,
                "bass": 0.6,
                "harmony": 0.2,
                "melody": 0.4,
                "atmosphere": 0.1
            }
        }
        
        return mix_levels.get(genre, mix_levels["house"])

    def _apply_final_processing(
        self, composition: np.ndarray, sample_rate: int, genre: str, energy: float
    ) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        max_val = np.max(np.abs(composition))
        if max_val > 0:
            composition = composition / max_val * 0.8  # –û—Å—Ç–∞–≤–ª—è–µ–º headroom
        
        # –õ—ë–≥–∫–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è
        threshold = 0.6
        ratio = 2.0
        
        compressed = composition.copy()
        over_threshold = np.abs(compressed) > threshold
        compressed[over_threshold] = np.sign(compressed[over_threshold]) * (
            threshold + (np.abs(compressed[over_threshold]) - threshold) / ratio
        )
        
        # Fade in/out
        fade_samples = sample_rate // 2  # 0.5 second fade
        
        if len(compressed) > fade_samples * 2:
            fade_in = np.linspace(0, 1, fade_samples)
            fade_out = np.linspace(1, 0, fade_samples)
            
            compressed[:fade_samples] *= fade_in
            compressed[-fade_samples:] *= fade_out
        
        return compressed

    def _create_emergency_audio(self, duration: int) -> bytes:
        """–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ"""
        
        self.logger.warning("üö® –°–æ–∑–¥–∞—ë–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ")
        
        sample_rate = 44100
        total_samples = int(duration * sample_rate)
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ–ª–æ–¥–∏—è —Å —Ä–∏—Ç–º–æ–º
        t = np.linspace(0, duration, total_samples)
        
        # –ú–µ–ª–æ–¥–∏—è
        melody = np.sin(2 * np.pi * 440 * t)  # A4
        melody += np.sin(2 * np.pi * 330 * t) * 0.5  # E4
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ä–∏—Ç–º
        beat_freq = 2  # 2 Hz = 120 BPM
        rhythm = np.sin(2 * np.pi * beat_freq * t) > 0
        rhythm = rhythm.astype(float)
        
        # –ë–∞—Å–æ–≤–∞—è –Ω–æ—Ç–∞
        bass = np.sin(2 * np.pi * 110 * t) * 0.6  # A2
        
        # –ú–∏–∫—à–∏—Ä—É–µ–º
        emergency_audio = melody * 0.3 + bass * 0.4 + rhythm * 0.1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        emergency_audio = emergency_audio / np.max(np.abs(emergency_audio)) * 0.7
        
        # Fade in/out
        fade_samples = sample_rate
        fade_in = np.linspace(0, 1, fade_samples)
        fade_out = np.linspace(1, 0, fade_samples)
        
        emergency_audio[:fade_samples] *= fade_in
        emergency_audio[-fade_samples:] *= fade_out
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ bytes
        buffer = io.BytesIO()
        sf.write(buffer, emergency_audio, sample_rate, format='WAV')
        audio_bytes = buffer.getvalue()
        buffer.close()
        
        self.logger.warning(f"üö® –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –∞—É–¥–∏–æ: {len(audio_bytes)} bytes")
        return audio_bytes

    def _enhance_prompt_for_genre(self, prompt: str, genre: Optional[str]) -> str:
        """–£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è MusicGen —Å —É—á–µ—Ç–æ–º –∂–∞–Ω—Ä–∞"""
        if not genre:
            return prompt

        genre_enhancements = {
            "trap": "heavy 808s, tight snares, dark atmosphere, urban vibes",
            "lofi": "warm analog sound, vinyl texture, mellow vibes, nostalgic",
            "dnb": "fast breakbeats, heavy bass, energetic, liquid",
            "house": "four on the floor, groovy bassline, soulful, danceable",
            "ambient": "ethereal pads, spacious reverb, peaceful, meditative",
            "techno": "driving four-on-the-floor, hypnotic, industrial sounds",
            "dubstep": "wobble bass, aggressive drops, syncopated drums",
            "rock": "electric guitars, driving drums, powerful, energetic"
        }

        enhancement = genre_enhancements.get(genre.lower(), "")
        if enhancement:
            enhanced = f"{prompt}, {enhancement}"
            self.logger.debug(f"Enhanced prompt: {enhanced}")
            return enhanced
        
        return prompt

    def get_model_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return {
            "available": self.MUSICGEN_AVAILABLE,
            "model_name": self.model_name if self.MUSICGEN_AVAILABLE else "fallback_generator",
            "device": self.device,
            "sample_rate": self.model.sample_rate if self.model else 44100,
            "max_duration": 30 if self.model else "unlimited"
        }

    async def test_generation(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"""
        try:
            test_audio = await self.generate(
                prompt="test electronic music",
                duration=5,
                temperature=0.8
            )
            
            return len(test_audio) > 1000  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª—Å—è —Ñ–∞–π–ª
            
        except Exception as e:
            self.logger.error(f"–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ –ø—Ä–æ—à–µ–ª: {e}")
            return False
