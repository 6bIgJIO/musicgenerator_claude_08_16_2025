# sample_engine.py - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫ –ø–æ–¥–±–æ—Ä–∞ —Å—ç–º–ø–ª–æ–≤
import re
import os
import json
import logging
import numpy as np
import asyncio
import pickle
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
import time

# ML –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    SEMANTIC_AVAILABLE = True
except ImportError:
    SEMANTIC_AVAILABLE = False
    logging.warning("Semantic analysis libraries not available")

# –ê—É–¥–∏–æ –∞–Ω–∞–ª–∏–∑
import librosa
import soundfile as sf
from pydub import AudioSegment, effects
from pydub.effects import compress_dynamic_range, normalize

from config import config

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∂–∞–Ω—Ä–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
GENRE_KEYWORDS = {
    "trap": ["trap", "drill", "phonk"],
    "lofi": ["lofi", "chill", "jazz", "vintage"],
    "house": ["house", "tech", "deep"],
    "ambient": ["ambient", "pad", "atmosphere"],
    "hip_hop": ["hip", "hop", "boom", "bap"],
    "dnb": ["dnb", "drum", "bass", "jungle"],
    "dubstep": ["dubstep", "wobble", "drop"],
    "techno": ["techno", "industrial", "acid"]
}


@dataclass
class SampleMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—ç–º–ø–ª–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏"""
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    path: str
    filename: str
    duration: float
    tempo: int
    key: Optional[str]
    tags: List[str]
    genres: List[str]
    instrument_role: Optional[str]

    # –ê—É–¥–∏–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    spectral_centroid: float
    spectral_rolloff: float
    zero_crossing_rate: float
    mfcc_features: np.ndarray
    chroma_features: np.ndarray

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (—Å default'–∞–º–∏)
    relative_path: Optional[str] = None
    category: Optional[str] = None
    semantic_embedding: Optional[np.ndarray] = None
    semantic_cluster: Optional[int] = None
    similarity_tags: Optional[List[str]] = None
    quality_score: float = 0.0
    energy_level: float = 0.0
    brightness: float = 0.0
    rhythmic_complexity: float = 0.0


class EnhancedSamplePicker:
    def __init__(self, sample_dir, index_file="enhanced_sample_index.json"):
        self.sample_dir = sample_dir
        self.index_file = os.path.join(sample_dir, index_file)

    def analyze_filename_advanced(self, filename):
        name = filename.lower()
        tags = []
        bmp = 120  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∞: bmp -> bpm
        key = None

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è BPM
        bpm_patterns = [r'(\d{2,3})bpm', r'(\d{2,3})_bpm', r'bmp(\d{2,3})']
        for pattern in bpm_patterns:
            match = re.search(pattern, name)
            if match:
                bmp = int(match.group(1))
                break

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è key
        key_patterns = [r'([a-g][#b]?)[_-]?maj', r'([a-g][#b]?)[_-]?min']
        for pattern in key_patterns:
            match = re.search(pattern, name)
            if match:
                key = match.group(1).upper()
                break

        return tags, bmp, key


class SemanticSampleEngine:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Å—ç–º–ø–ª–æ–≤
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ SBERT
    - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö —Å—ç–º–ø–ª–æ–≤  
    - MFCC/—Ö—Ä–æ–º–∞ –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∞—É–¥–∏–æ-–ø–æ–¥–æ–±–∏—è
    - –ñ–∞–Ω—Ä–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    - –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    - –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
    """
    
    def __init__(self, sample_dir: str = None):
        self.sample_dir = sample_dir or getattr(config, 'DEFAULT_SAMPLE_DIR', './samples')
        self.logger = logging.getLogger(__name__)
        self.enhanced_picker = EnhancedSamplePicker(self.sample_dir)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
        self.semantic_model = None
        self.embeddings_cache = {}
        self.sample_clusters = {}
        
        if SEMANTIC_AVAILABLE:
            self._init_semantic_model()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å
        self.samples_index: List[SampleMetadata] = []
        self.load_or_build_index()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.performance_stats = {
            "queries": 0,
            "cache_hits": 0,
            "avg_query_time": 0.0
        }

    def _init_semantic_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            model_name = getattr(config, 'SAMPLE_MATCHING', {}).get("embedding_model", 
                                                  "sentence-transformers/all-MiniLM-L6-v2")
            self.semantic_model = SentenceTransformer(model_name)
            self.logger.info(f"‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏: {e}")
            self.semantic_model = None

    def analyze_audio_content(self, full_path: str) -> Dict:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ñ–∞–π–ª–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ librosa
            max_duration = getattr(config, 'AUDIO_ANALYSIS', {}).get('max_duration', 30)
            sample_rate = getattr(config, 'AUDIO_ANALYSIS', {}).get('sample_rate', 22050)
            
            y, sr = librosa.load(full_path, duration=max_duration, sr=sample_rate)
            
            analysis = {
                "tempo": 120,
                "key": None,
                "content_tags": [],
                "spectral_centroid": 0.0,
                "spectral_rolloff": 0.0,
                "zero_crossing_rate": 0.0,
                "brightness": 0.0,
                "energy": 0.5,
                "rhythmic_complexity": 0.0,
                "mfcc": np.array([]),
                "chroma": np.array([])
            }
            
            if len(y) > 0:
                # –¢–µ–º–ø
                tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
                analysis["tempo"] = float(tempo)
                
                # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
                analysis["spectral_centroid"] = float(spectral_centroid.mean())
                
                spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
                analysis["spectral_rolloff"] = float(spectral_rolloff.mean())
                
                zcr = librosa.feature.zero_crossing_rate(y)
                analysis["zero_crossing_rate"] = float(zcr.mean())
                
                analysis["brightness"] = analysis["spectral_centroid"] / sr
                
                # MFCC
                n_mfcc = getattr(config, 'AUDIO_ANALYSIS', {}).get('n_mfcc', 13)
                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
                analysis["mfcc"] = mfcc.mean(axis=1)
                
                # –•—Ä–æ–º–∞
                chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
                analysis["chroma"] = chroma.mean(axis=1)
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Ö—Ä–æ–º–∞
                if chroma.size > 0:
                    chroma_mean = chroma.mean(axis=1)
                    key_idx = chroma_mean.argmax()
                    keys = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
                    analysis["key"] = keys[key_idx]
                
                # –≠–Ω–µ—Ä–≥–∏—è
                rms = librosa.feature.rms(y=y)
                analysis["energy"] = float(min(1.0, rms.mean() * 10))
                
                # –†–∏—Ç–º–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
                onset_frames = librosa.onset.onset_detect(y=y, sr=sr)
                analysis["rhythmic_complexity"] = len(onset_frames) / (len(y) / sr)
                
                # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ —Ç–µ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ–∞–Ω–∞–ª–∏–∑–∞
                if analysis["energy"] > 0.7:
                    analysis["content_tags"].append("energetic")
                elif analysis["energy"] < 0.3:
                    analysis["content_tags"].append("calm")
                    
                if analysis["brightness"] > 0.3:
                    analysis["content_tags"].append("bright")
                elif analysis["brightness"] < 0.1:
                    analysis["content_tags"].append("dark")
                    
                if analysis["rhythmic_complexity"] > 5:
                    analysis["content_tags"].append("complex")
            
            return analysis
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ {full_path}: {e}")
            return {
                "tempo": 120,
                "key": None,
                "content_tags": [],
                "spectral_centroid": 0.0,
                "spectral_rolloff": 0.0,
                "zero_crossing_rate": 0.0,
                "brightness": 0.0,
                "energy": 0.5,
                "rhythmic_complexity": 0.0,
                "mfcc": np.array([]),
                "chroma": np.array([])
            }

    def build_enhanced_index(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å—ç–º–ø–ª–æ–≤ —Å –∂–∞–Ω—Ä–æ–≤–æ–π –ø—Ä–∏–≤—è–∑–∫–æ–π"""
        logging.info("üîç –ù–∞—á–∏–Ω–∞—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å—ç–º–ø–ª–æ–≤...")
        enhanced_index = []
        processed = 0
        
        for root, _, files in os.walk(self.sample_dir):
            for file in files:
                if file.lower().endswith(('.wav', '.mp3', '.aiff', '.flac')):
                    full_path = os.path.join(root, file)
                    try:
                        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                        try:
                            audio = AudioSegment.from_file(full_path)
                            duration = len(audio) / 1000
                        except:
                            # Fallback —á–µ—Ä–µ–∑ librosa
                            y, sr = librosa.load(full_path, duration=30)
                            duration = len(y) / sr
                        
                        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                        min_duration = getattr(config, 'QUALITY_FILTERS', {}).get('min_duration', 0.5)
                        max_duration = getattr(config, 'QUALITY_FILTERS', {}).get('max_duration', 300)
                        
                        if duration < min_duration or duration > max_duration:
                            continue
                        
                        # –ê–Ω–∞–ª–∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                        filename_tags, filename_bmp, filename_key = self.enhanced_picker.analyze_filename_advanced(file)
                        
                        # –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        audio_analysis = self.analyze_audio_content(full_path)
                        
                        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ–ø–µ—á–∞—Ç–∫–∞ filename_bmp -> filename_bpm
                        final_bpm = filename_bmp if filename_bmp != 120 else audio_analysis["tempo"]
                        final_key = filename_key or audio_analysis["key"]
                        all_tags = list(set(filename_tags + audio_analysis["content_tags"]))
                        
                        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        category = "loop" if duration > 8 else "oneshot"
                        if "loop" in filename_tags or "loop" in file.lower():
                            category = "loop"
                        
                        # –ñ–∞–Ω—Ä–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –ø—É—Ç–∏
                        path_lower = full_path.lower()
                        detected_genres = []
                        for genre, keywords in GENRE_KEYWORDS.items():
                            for keyword in keywords[:3]:  # –¢–æ–ø-3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
                                if keyword in path_lower:
                                    detected_genres.append(genre)
                                    break
                        
                        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ä–æ–ª–∏
                        instrument_role = self._detect_instrument_role(file, audio_analysis)
                        
                        # –°–æ–∑–¥–∞–µ–º SampleMetadata –æ–±—ä–µ–∫—Ç
                        metadata = SampleMetadata(
                            path=full_path,
                            filename=file,
                            duration=round(duration, 3),
                            tempo=round(final_bpm),
                            key=final_key,
                            tags=all_tags,
                            genres=detected_genres,
                            instrument_role=instrument_role,
                            spectral_centroid=audio_analysis["spectral_centroid"],
                            spectral_rolloff=audio_analysis["spectral_rolloff"],
                            zero_crossing_rate=audio_analysis["zero_crossing_rate"],
                            mfcc_features=audio_analysis["mfcc"],
                            chroma_features=audio_analysis["chroma"],
                            relative_path=os.path.relpath(full_path, self.sample_dir),
                            category=category,
                            quality_score=self._calculate_quality_score(audio_analysis, all_tags),
                            energy_level=audio_analysis["energy"],
                            brightness=audio_analysis["brightness"],
                            rhythmic_complexity=audio_analysis["rhythmic_complexity"]
                        )
                        
                        enhanced_index.append(metadata)
                        processed += 1
                        
                        if processed % 50 == 0:
                            logging.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} —Ñ–∞–π–ª–æ–≤")
                    
                    except Exception as e:
                        logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file}: {e}")
        
        logging.info(f"üéØ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(enhanced_index)} —Å—ç–º–ø–ª–æ–≤")
        return enhanced_index

    def _detect_instrument_role(self, filename: str, audio_analysis: Dict) -> Optional[str]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ä–æ–ª–∏"""
        name_lower = filename.lower()
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∏–º–µ–Ω–∏
        if any(word in name_lower for word in ['kick', 'bd', 'bassdrum']):
            return 'kick'
        elif any(word in name_lower for word in ['snare', 'sd']):
            return 'snare'
        elif any(word in name_lower for word in ['hihat', 'hh', 'hat']):
            return 'hihat'
        elif any(word in name_lower for word in ['bass', 'sub']):
            return 'bass'
        elif any(word in name_lower for word in ['lead', 'melody']):
            return 'lead'
        elif any(word in name_lower for word in ['pad', 'chord']):
            return 'pad'
        elif any(word in name_lower for word in ['fx', 'effect', 'riser']):
            return 'fx'
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∞—É–¥–∏–æ—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
        energy = audio_analysis.get("energy", 0.5)
        brightness = audio_analysis.get("brightness", 0.0)
        centroid = audio_analysis.get("spectral_centroid", 0)
        zcr = audio_analysis.get("zero_crossing_rate", 0)
        
        # –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if centroid < 500:
            return "kick" if energy > 0.6 else "bass"
        elif centroid < 1500:
            return "snare" if energy > 0.5 and zcr > 0.1 else "bass"
        elif centroid < 4000:
            return "lead" if energy > 0.4 else "pad"
        else:
            return "hihat" if zcr > 0.2 else "fx"

    def _calculate_quality_score(self, audio_analysis: Dict, tags: List[str]) -> float:
        """–†–∞—Å—á—ë—Ç —Å–∫–æ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—ç–º–ø–ª–∞"""
        score = 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–µ–≥–æ–≤
        if len(tags) > 0:
            score += 0.2
        if len(tags) > 2:
            score += 0.2
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ
        if audio_analysis.get("spectral_centroid", 0) > 0:
            score += 0.2
        
        # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–∞–Ω—Å
        energy = audio_analysis.get("energy", 0)
        if 0.1 < energy < 0.9:  # –ù–µ —Å–ª–∏—à–∫–æ–º —Ç–∏—Ö–æ –∏ –Ω–µ –∫–ª–∏–ø–ø–∏–Ω–≥
            score += 0.2
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if audio_analysis.get("spectral_rolloff", 0) > 0:
            score += 0.2
        
        return min(1.0, score)

    def load_or_build_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        index_file = getattr(config, 'ENHANCED_INDEX_FILE', 'enhanced_sample_index.json')
        index_path = Path(self.sample_dir) / index_file
        semantic_cache_file = getattr(config, 'SEMANTIC_CACHE_FILE', 'semantic_cache.pkl')
        semantic_cache_path = Path(self.sample_dir) / semantic_cache_file
        
        if index_path.exists():
            self.logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Å—ç–º–ø–ª–æ–≤ –∏–∑ {index_path}")
            try:
                with open(index_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.samples_index = []
                for item in data:
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ SampleMetadata
                    metadata = SampleMetadata(
                        path=item.get("path", ""),
                        filename=item.get("filename", ""),
                        duration=item.get("duration", 0.0),
                        tempo=item.get("tempo", 120),
                        key=item.get("key"),
                        tags=item.get("tags", []),
                        genres=item.get("genres", []),
                        instrument_role=item.get("instrument_role"),
                        spectral_centroid=item.get("spectral_centroid", 0.0),
                        spectral_rolloff=item.get("spectral_rolloff", 0.0),
                        zero_crossing_rate=item.get("zero_crossing_rate", 0.0),
                        mfcc_features=np.array(item.get("mfcc_features", [])),
                        chroma_features=np.array(item.get("chroma_features", [])),
                        relative_path=item.get("relative_path"),
                        category=item.get("category"),
                        quality_score=item.get("quality_score", 0.0),
                        energy_level=item.get("energy_level", 0.0),
                        brightness=item.get("brightness", 0.0),
                        rhythmic_complexity=item.get("rhythmic_complexity", 0.0)
                    )
                    self.samples_index.append(metadata)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                if SEMANTIC_AVAILABLE and semantic_cache_path.exists():
                    try:
                        with open(semantic_cache_path, 'rb') as f:
                            semantic_cache = pickle.load(f)
                        
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ embedding –∫ —Å—ç–º–ø–ª–∞–º
                        embeddings = semantic_cache.get("embeddings", [])
                        clusters = semantic_cache.get("clusters", [])
                        
                        for i, sample in enumerate(self.samples_index):
                            if i < len(embeddings):
                                sample.semantic_embedding = embeddings[i]
                            if i < len(clusters):
                                sample.semantic_cluster = clusters[i]
                        
                        self.logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(embeddings)} —Å—ç–º–ø–ª–æ–≤")
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫—ç—à–∞: {e}")
                
                self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.samples_index)} —Å—ç–º–ø–ª–æ–≤")
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                self.build_semantic_index()
        else:
            self.logger.info("‚öôÔ∏è –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π")
            self.build_semantic_index()

    def build_semantic_index(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        self.logger.info("üß† –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ —Å—ç–º–ø–ª–æ–≤...")
        
        # –°–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∏–º –±–∞–∑–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        self.samples_index = self.build_enhanced_index()
        
        # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ embedding –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if SEMANTIC_AVAILABLE and self.semantic_model and self.samples_index:
            self.logger.info("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö embedding...")
            
            try:
                # –°–æ–∑–¥–∞—ë–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                semantic_texts = []
                for sample in self.samples_index:
                    semantic_text = self._create_semantic_text(sample)
                    semantic_texts.append(semantic_text)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
                embeddings = self.semantic_model.encode(semantic_texts, show_progress_bar=True)
                
                # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
                n_clusters = min(50, len(self.samples_index) // 10)
                if n_clusters > 1:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                    clusters = kmeans.fit_predict(embeddings)
                else:
                    clusters = [0] * len(embeddings)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ —Å—ç–º–ø–ª–∞–º
                for i, (sample, embedding, cluster) in enumerate(zip(self.samples_index, embeddings, clusters)):
                    sample.semantic_embedding = embedding
                    sample.semantic_cluster = int(cluster)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à
                semantic_cache = {
                    "embeddings": embeddings,
                    "clusters": clusters,
                    "model_name": getattr(config, 'SAMPLE_MATCHING', {}).get("embedding_model", "default")
                }
                
                semantic_cache_file = getattr(config, 'SEMANTIC_CACHE_FILE', 'semantic_cache.pkl')
                semantic_cache_path = Path(self.sample_dir) / semantic_cache_file
                with open(semantic_cache_path, 'wb') as f:
                    pickle.dump(semantic_cache, f)
                
                self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö embedding, {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö embedding: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
        self.save_index()
        
        self.logger.info(f"üéØ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(self.samples_index)} —Å—ç–º–ø–ª–æ–≤")

    def _create_semantic_text(self, sample: SampleMetadata) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è embedding"""
        text_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏
        text_parts.extend(sample.tags)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∂–∞–Ω—Ä—ã
        text_parts.extend(sample.genres)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
        if sample.instrument_role:
            text_parts.append(sample.instrument_role)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —ç–Ω–µ—Ä–≥–∏–∏
        if sample.energy_level > 0.7:
            text_parts.append("high-energy aggressive powerful")
        elif sample.energy_level < 0.3:
            text_parts.append("low-energy calm peaceful")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        if sample.brightness > 0.3:
            text_parts.append("bright")
        elif sample.brightness < 0.1:
            text_parts.append("dark")
        
        return " ".join(text_parts)

    def save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ JSON"""
        index_file = getattr(config, 'ENHANCED_INDEX_FILE', 'enhanced_sample_index.json')
        index_path = Path(self.sample_dir) / index_file
        
        try:
            index_data = []
            for sample in self.samples_index:
                item = {
                    "path": sample.path,
                    "filename": sample.filename,
                    "duration": sample.duration,
                    "tempo": sample.tempo,
                    "key": sample.key,
                    "tags": sample.tags,
                    "genres": sample.genres,
                    "instrument_role": sample.instrument_role,
                    "spectral_centroid": sample.spectral_centroid,
                    "spectral_rolloff": sample.spectral_rolloff,
                    "zero_crossing_rate": sample.zero_crossing_rate,
                    "mfcc_features": sample.mfcc_features.tolist() if sample.mfcc_features.size > 0 else [],
                    "chroma_features": sample.chroma_features.tolist() if sample.chroma_features.size > 0 else [],
                    "relative_path": sample.relative_path,
                    "category": sample.category,
                    "quality_score": sample.quality_score,
                    "energy_level": sample.energy_level,
                    "brightness": sample.brightness,
                    "rhythmic_complexity": sample.rhythmic_complexity
                }
                index_data.append(item)
            
            with open(index_path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_path} ({len(index_data)} —Å—ç–º–ø–ª–æ–≤)")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")

    async def find_samples(
        self,
        tags: List[str],
        instruments: Optional[List[str]] = None,
        genre: Optional[str] = None,
        bpm: Optional[int] = None,
        energy: float = 0.5,
        max_results: int = 10,
        min_quality: float = 0.3
    ) -> List[Dict]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å—ç–º–ø–ª–æ–≤
        
        Args:
            tags: –°–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            instruments: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
            genre: –¶–µ–ª–µ–≤–æ–π –∂–∞–Ω—Ä
            bpm: –¶–µ–ª–µ–≤–æ–π BPM
            energy: –£—Ä–æ–≤–µ–Ω—å —ç–Ω–µ—Ä–≥–∏–∏ (0-1)
            max_results: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_quality: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (0-1)
        """
        start_time = time.time()
        self.performance_stats["queries"] += 1
        
        self.logger.info(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: —Ç–µ–≥–∏={tags}, –∂–∞–Ω—Ä={genre}, BPM={bpm}")
        
        # –°–æ–∑–¥–∞—ë–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        search_query = {
            "tags": tags,
            "instruments": instruments or [],
            "genre": genre,
            "bpm": bpm,
            "energy": energy,
            "min_quality": min_quality
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        query_hash = self._hash_query(search_query)
        if query_hash in self.embeddings_cache:
            self.performance_stats["cache_hits"] += 1
            self.logger.info("  üí® –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞")
            return self.embeddings_cache[query_hash][:max_results]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        candidates = await self._search_candidates(search_query)
        scored_samples = await self._score_samples(candidates, search_query)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        filtered_samples = [
            sample for sample in scored_samples 
            if sample["score"] >= min_quality and sample["metadata"].quality_score >= min_quality
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        filtered_samples.sort(key=lambda x: x["score"], reverse=True)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        diversified_samples = self._diversify_results(filtered_samples, max_results)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.embeddings_cache[query_hash] = diversified_samples
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        query_time = time.time() - start_time
        self.performance_stats["avg_query_time"] = (
            (self.performance_stats["avg_query_time"] * (self.performance_stats["queries"] - 1) + query_time) 
            / self.performance_stats["queries"]
        )
        
        self.logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(diversified_samples)} —Å—ç–º–ø–ª–æ–≤ –∑–∞ {query_time:.2f}—Å")
        
        return diversified_samples

    async def _search_candidates(self, query: Dict) -> List[SampleMetadata]:
        """–ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        candidates = []
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∂–∞–Ω—Ä—É
        if query["genre"]:
            genre_samples = [s for s in self.samples_index if query["genre"] in s.genres]
            if len(genre_samples) > 100:  # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Å—ç–º–ø–ª–æ–≤ –∂–∞–Ω—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                base_samples = genre_samples
            else:
                base_samples = self.samples_index
        else:
            base_samples = self.samples_index
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ BPM
        if query["bpm"]:
            tempo_tolerance = getattr(config, 'SAMPLE_MATCHING', {}).get("tempo_tolerance", 15)
            bpm_range = (query["bpm"] - tempo_tolerance, query["bpm"] + tempo_tolerance)
            base_samples = [
                s for s in base_samples 
                if bpm_range[0] <= s.tempo <= bpm_range[1]
            ]
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        min_quality = query["min_quality"]
        base_samples = [s for s in base_samples if s.quality_score >= min_quality]
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —ç–Ω–µ—Ä–≥–∏–∏ (¬±0.3 –æ—Ç —Ü–µ–ª–µ–≤–æ–π)
        target_energy = query["energy"]
        energy_tolerance = 0.3
        base_samples = [
            s for s in base_samples 
            if abs(s.energy_level - target_energy) <= energy_tolerance
        ]
        
        self.logger.info(f"  üìã –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(base_samples)}")
        
        return base_samples

    async def _score_samples(self, candidates: List[SampleMetadata], query: Dict) -> List[Dict]:
        """–°–∫–æ—Ä–∏–Ω–≥ —Å—ç–º–ø–ª–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –∏ –∞—É–¥–∏–æ-–ø–æ–¥–æ–±–∏—é"""
        scored_samples = []
        
        # –°–æ–∑–¥–∞—ë–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        semantic_query = self._create_semantic_query(query)
        
        for sample in candidates:
            score = await self._calculate_sample_score(sample, query, semantic_query)
            
            scored_samples.append({
                "metadata": sample,
                "score": score,
                "path": sample.path,
                "filename": sample.filename,
                "instrument_role": sample.instrument_role,
                "tags": sample.tags,
                "tempo": sample.tempo,
                "spectral_centroid": sample.spectral_centroid,
                "spectral_rolloff": sample.spectral_rolloff,
                "zero_crossing_rate": sample.zero_crossing_rate,
                "mfcc_features": sample.mfcc_features.tolist() if sample.mfcc_features.size > 0 else [],
                "chroma_features": sample.chroma_features.tolist() if sample.chroma_features.size > 0 else [],
                "quality_score": sample.quality_score,
                "energy_level": sample.energy_level,
                "brightness": sample.brightness,
                "rhythmic_complexity": sample.rhythmic_complexity,
                "semantic_cluster": sample.semantic_cluster
            })
        
        return scored_samples

    async def _calculate_sample_score(
        self, sample: SampleMetadata, query: Dict, semantic_query: str
    ) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ —Å—ç–º–ø–ª–∞"""
        score_components = {}
        
        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–æ—Ä (40%)
        if self.semantic_model and sample.semantic_embedding is not None:
            semantic_score = self._calculate_semantic_similarity(sample, semantic_query)
            score_components["semantic"] = semantic_score * 0.4
        else:
            # Fallback: —Å–∫–æ—Ä –ø–æ —Ç–µ–≥–∞–º
            tag_score = self._calculate_tag_similarity(sample.tags, query["tags"])
            score_components["semantic"] = tag_score * 0.4
        
        # 2. –¢–µ–º–ø–æ–≤–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (15%)
        if query["bpm"]:
            tempo_score = self._calculate_tempo_similarity(sample.tempo, query["bpm"])
            score_components["tempo"] = tempo_score * 0.15
        else:
            score_components["tempo"] = 0.1  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –µ—Å–ª–∏ BPM –Ω–µ –≤–∞–∂–µ–Ω
        
        # 3. –ñ–∞–Ω—Ä–æ–≤–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (15%)
        if query["genre"]:
            genre_score = 1.0 if query["genre"] in sample.genres else 0.3
            score_components["genre"] = genre_score * 0.15
        else:
            score_components["genre"] = 0.1
        
        # 4. –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (10%)
        energy_score = 1.0 - abs(sample.energy_level - query["energy"])
        score_components["energy"] = max(0, energy_score) * 0.1
        
        # 5. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (10%)
        instrument_score = 0.0
        if query["instruments"]:
            if sample.instrument_role in query["instruments"]:
                instrument_score = 1.0
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
                for instrument in query["instruments"]:
                    if any(instrument.lower() in tag.lower() for tag in sample.tags):
                        instrument_score = 0.7
                        break
        score_components["instrument"] = instrument_score * 0.1
        
        # 6. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –±–æ–Ω—É—Å (10%)
        quality_bonus = sample.quality_score * 0.1
        score_components["quality"] = quality_bonus
        
        # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä
        total_score = sum(score_components.values())
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø —Å—ç–º–ø–ª–æ–≤)
        if total_score > 0.6:
            self.logger.debug(f"    üéØ {sample.filename}: {total_score:.2f} "
                            f"(sem:{score_components['semantic']:.2f}, "
                            f"tempo:{score_components['tempo']:.2f}, "
                            f"genre:{score_components['genre']:.2f})")
        
        return total_score

    def _calculate_semantic_similarity(self, sample: SampleMetadata, semantic_query: str) -> float:
        """–†–∞—Å—á—ë—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ SBERT"""
        if not self.semantic_model or sample.semantic_embedding is None:
            return 0.0
        
        try:
            # –≠–Ω–∫–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            query_embedding = self.semantic_model.encode([semantic_query])
            
            # –í—ã—á–∏—Å–ª—è–µ–º cosine similarity
            similarity = cosine_similarity(
                query_embedding, 
                sample.semantic_embedding.reshape(1, -1)
            )[0][0]
            
            return float(similarity)
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
            return 0.0

    def _calculate_tag_similarity(self, sample_tags: List[str], query_tags: List[str]) -> float:
        """Fallback —Ä–∞—Å—á—ë—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ —Ç–µ–≥–∞–º"""
        if not query_tags or not sample_tags:
            return 0.0
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        sample_tags_lower = [tag.lower() for tag in sample_tags]
        query_tags_lower = [tag.lower() for tag in query_tags]
        
        # –ü—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        direct_matches = len(set(sample_tags_lower) & set(query_tags_lower))
        
        # –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–ø–æ–¥—Å—Ç—Ä–æ–∫–∏)
        partial_matches = 0
        for sample_tag in sample_tags_lower:
            for query_tag in query_tags_lower:
                if query_tag in sample_tag or sample_tag in query_tag:
                    partial_matches += 0.5
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        total_score = (direct_matches + partial_matches) / len(query_tags_lower)
        
        return min(1.0, total_score)

    def _calculate_tempo_similarity(self, sample_tempo: int, target_tempo: int) -> float:
        """–†–∞—Å—á—ë—Ç —Ç–µ–º–ø–æ–≤–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è"""
        tempo_diff = abs(sample_tempo - target_tempo)
        tolerance = getattr(config, 'SAMPLE_MATCHING', {}).get("tempo_tolerance", 15)
        
        if tempo_diff <= 3:
            return 1.0
        elif tempo_diff <= tolerance:
            return 1.0 - (tempo_diff / tolerance)
        else:
            return 0.0

    def _create_semantic_query(self, query: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è SBERT"""
        query_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏
        if query["tags"]:
            query_parts.append(" ".join(query["tags"]))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        if query["instruments"]:
            query_parts.append(" ".join(query["instruments"]))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∂–∞–Ω—Ä
        if query["genre"]:
            query_parts.append(query["genre"])
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —ç–Ω–µ—Ä–≥–∏–∏
        energy_level = query.get("energy", 0.5)
        if energy_level > 0.7:
            query_parts.append("energetic high-energy aggressive")
        elif energy_level < 0.3:
            query_parts.append("calm peaceful soft quiet")
        else:
            query_parts.append("medium-energy balanced")
        
        return " ".join(query_parts)

    def _diversify_results(self, samples: List[Dict], max_results: int) -> List[Dict]:
        """–î–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–æ–≤"""
        if len(samples) <= max_results:
            return samples
        
        diversified = []
        used_instruments = set()
        used_clusters = set()
        
        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: –±–µ—Ä—ë–º –ª—É—á—à–∏–µ —Å—ç–º–ø–ª—ã —Ä–∞–∑–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        for sample in samples:
            if len(diversified) >= max_results:
                break
            
            instrument = sample.get("instrument_role")
            cluster = getattr(sample["metadata"], "semantic_cluster", None)
            
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º —Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –∫–ª–∞—Å—Ç–µ—Ä—ã
            if instrument not in used_instruments or cluster not in used_clusters:
                diversified.append(sample)
                used_instruments.add(instrument)
                used_clusters.add(cluster)
        
        # –í—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥: –¥–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ª—É—á—à–∏–µ —Å—ç–º–ø–ª—ã
        for sample in samples:
            if len(diversified) >= max_results:
                break
            if sample not in diversified:
                diversified.append(sample)
        
        return diversified[:max_results]

    def _hash_query(self, query: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"""
        import hashlib
        query_str = json.dumps(query, sort_keys=True)
        return hashlib.md5(query_str.encode()).hexdigest()

    def get_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        if not self.samples_index:
            return {"total_samples": 0}
        
        stats = {
            "total_samples": len(self.samples_index),
            "avg_quality": sum(s.quality_score for s in self.samples_index) / len(self.samples_index),
            "genre_distribution": {},
            "instrument_distribution": {},
            "tempo_ranges": {"slow": 0, "medium": 0, "fast": 0},
            "energy_distribution": {"low": 0, "medium": 0, "high": 0},
            "performance_stats": self.performance_stats
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∂–∞–Ω—Ä–∞–º
        for sample in self.samples_index:
            for genre in sample.genres:
                stats["genre_distribution"][genre] = stats["genre_distribution"].get(genre, 0) + 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º
        for sample in self.samples_index:
            if sample.instrument_role:
                role = sample.instrument_role
                stats["instrument_distribution"][role] = stats["instrument_distribution"].get(role, 0) + 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–º–ø—É
        for sample in self.samples_index:
            if sample.tempo < 100:
                stats["tempo_ranges"]["slow"] += 1
            elif sample.tempo < 140:
                stats["tempo_ranges"]["medium"] += 1
            else:
                stats["tempo_ranges"]["fast"] += 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —ç–Ω–µ—Ä–≥–∏–∏
        for sample in self.samples_index:
            if sample.energy_level < 0.4:
                stats["energy_distribution"]["low"] += 1
            elif sample.energy_level < 0.7:
                stats["energy_distribution"]["medium"] += 1
            else:
                stats["energy_distribution"]["high"] += 1
        
        return stats


# ===== –°–ò–°–¢–ï–ú–ê –≠–§–§–ï–ö–¢–û–í =====

class EffectsProcessor:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"{__name__}.{name}")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∞ –∫ –∞—É–¥–∏–æ"""
        raise NotImplementedError


class EQProcessor(EffectsProcessor):
    """–≠–∫–≤–∞–ª–∞–π–∑–µ—Ä - —Ç—Ä—ë—Ö–ø–æ–ª–æ—Å–Ω—ã–π EQ"""
    
    def __init__(self):
        super().__init__("EQ")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ EQ
        params: {"low": gain_db, "mid": gain_db, "high": gain_db}
        """
        try:
            processed = audio
            
            low_gain = params.get("low", 0)
            mid_gain = params.get("mid", 0) 
            high_gain = params.get("high", 0)
            
            # –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã (–¥–æ 300Hz)
            if low_gain != 0:
                low_filtered = processed.low_pass_filter(300)
                if low_gain > 0:
                    low_filtered = low_filtered + low_gain
                else:
                    low_filtered = low_filtered.apply_gain(low_gain)
                
                high_pass = processed.high_pass_filter(300)
                processed = low_filtered.overlay(high_pass)
            
            # –í—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã (–æ—Ç 3000Hz)  
            if high_gain != 0:
                high_filtered = processed.high_pass_filter(3000)
                if high_gain > 0:
                    high_filtered = high_filtered + high_gain
                else:
                    high_filtered = high_filtered.apply_gain(high_gain)
                
                low_pass = processed.low_pass_filter(3000)
                processed = low_pass.overlay(high_filtered)
            
            # –°—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã (—Å–∏–º—É–ª—è—Ü–∏—è)
            if mid_gain != 0:
                processed = processed + (mid_gain * 0.5)  # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è
            
            self.logger.debug(f"EQ applied: L{low_gain:+.1f} M{mid_gain:+.1f} H{high_gain:+.1f}")
            return processed
            
        except Exception as e:
            self.logger.error(f"EQ processing error: {e}")
            return audio


class CompressorProcessor(EffectsProcessor):
    """–ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞"""
    
    def __init__(self):
        super().__init__("Compressor")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏
        params: {"ratio": float, "threshold": db, "attack": ms, "release": ms}
        """
        try:
            ratio = params.get("ratio", 2.0)
            threshold = params.get("threshold", -12)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –∫–æ–º–ø—Ä–µ—Å—Å–∏—é pydub –∫–∞–∫ –±–∞–∑—É
            if ratio > 1:
                # –ú—è–≥–∫–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                peak_level = audio.max_dBFS
                
                if peak_level > threshold:
                    over_threshold = peak_level - threshold
                    reduction = over_threshold - (over_threshold / ratio)
                    processed = audio - reduction
                    
                    self.logger.debug(f"Compression: {ratio}:1, reduction {reduction:.1f}dB")
                    return processed
            
            return audio
            
        except Exception as e:
            self.logger.error(f"Compression processing error: {e}")
            return audio


class ReverbProcessor(EffectsProcessor):
    """–†–µ–≤–µ—Ä–±–µ—Ä–∞—Ü–∏—è (—Å–∏–º—É–ª—è—Ü–∏—è —á–µ—Ä–µ–∑ –∑–∞–¥–µ—Ä–∂–∫—É –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é)"""
    
    def __init__(self):
        super().__init__("Reverb")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–≤–µ—Ä–±–∞
        params: {"room_size": 0-1, "wet_level": 0-1, "type": "hall/room/plate"}
        """
        try:
            room_size = params.get("room_size", 0.3)
            wet_level = params.get("wet_level", 0.2)
            reverb_type = params.get("type", "room")
            
            if wet_level <= 0:
                return audio
            
            # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç—É—é —Å–∏–º—É–ª—è—Ü–∏—é —Ä–µ–≤–µ—Ä–±–∞ —á–µ—Ä–µ–∑ –∑–∞–¥–µ—Ä–∂–∫–∏
            delays = []
            
            if reverb_type == "hall":
                delays = [(50, 0.3), (120, 0.2), (250, 0.15), (400, 0.1)]
            elif reverb_type == "plate":
                delays = [(20, 0.4), (60, 0.3), (140, 0.2)]
            else:  # room
                delays = [(30, 0.25), (80, 0.15), (150, 0.1)]
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –∫–æ–º–Ω–∞—Ç—ã
            scaled_delays = [(int(delay * (0.5 + room_size)), gain * wet_level) 
                           for delay, gain in delays]
            
            reverb_audio = AudioSegment.silent(duration=0)
            
            for delay_ms, gain in scaled_delays:
                if delay_ms > 0 and gain > 0:
                    delayed = AudioSegment.silent(duration=delay_ms) + audio
                    delayed = delayed.apply_gain(-20 * (1 - gain))  # –ó–∞—Ç—É—Ö–∞–Ω–∏–µ
                    
                    if len(reverb_audio) == 0:
                        reverb_audio = delayed
                    else:
                        reverb_audio = reverb_audio.overlay(delayed)
            
            # –°–º–µ—à–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª —Å —Ä–µ–≤–µ—Ä–±–æ–º
            if len(reverb_audio) > 0:
                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–Ω–æ–π –¥–ª–∏–Ω–µ
                max_len = max(len(audio), len(reverb_audio))
                if len(audio) < max_len:
                    audio = audio + AudioSegment.silent(duration=max_len - len(audio))
                if len(reverb_audio) < max_len:
                    reverb_audio = reverb_audio + AudioSegment.silent(duration=max_len - len(reverb_audio))
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–≤–µ—Ä–± –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–≤—É–∫–∞
                reverb_audio = reverb_audio.low_pass_filter(8000)
                
                result = audio.overlay(reverb_audio)
                
                self.logger.debug(f"Reverb applied: {reverb_type}, size {room_size}, wet {wet_level}")
                return result
            
            return audio
            
        except Exception as e:
            self.logger.error(f"Reverb processing error: {e}")
            return audio


class SaturationProcessor(EffectsProcessor):
    """–ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ (—Å–∏–º—É–ª—è—Ü–∏—è –∞–Ω–∞–ª–æ–≥–æ–≤–æ–≥–æ —Ç–µ–ø–ª–∞)"""
    
    def __init__(self):
        super().__init__("Saturation")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞—Å—ã—â–µ–Ω–∏—è
        params: {"amount": 0-1, "type": "tube/tape/transistor", "warmth": 0-1}
        """
        try:
            amount = params.get("amount", 0.0)
            saturation_type = params.get("type", "tube")
            warmth = params.get("warmth", 0.0)
            
            if amount <= 0:
                return audio
            
            processed = audio
            
            # –°–∏–º—É–ª—è—Ü–∏—è –Ω–∞—Å—ã—â–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ª—ë–≥–∫–æ–µ –ø–æ–¥–Ω—è—Ç–∏–µ –Ω–∏–∑–∫–∏—Ö —á–∞—Å—Ç–æ—Ç –∏ –∫–æ–º–ø—Ä–µ—Å—Å–∏—é
            if saturation_type == "tube":
                # –õ–∞–º–ø–æ–≤–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ - —Ç—ë–ø–ª–æ–µ, –º—è–≥–∫–æ–µ
                if warmth > 0:
                    # –ü–æ–¥–Ω–∏–º–∞–µ–º –Ω–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
                    low_boost = warmth * 2  # dB
                    low_freq = processed.low_pass_filter(800) + low_boost
                    high_freq = processed.high_pass_filter(800)
                    processed = low_freq.overlay(high_freq)
                    
                # –ú—è–≥–∫–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è
                if processed.max_dBFS > -6:
                    soft_compress = amount * 2
                    processed = processed - soft_compress
                    
            elif saturation_type == "tape":
                # –õ–µ–Ω—Ç–æ—á–Ω–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ - –≤–∏–Ω—Ç–∞–∂–Ω–æ–µ
                processed = processed.high_pass_filter(30).low_pass_filter(15000)  # –í–∏–Ω—Ç–∞–∂–Ω–∞—è –ø–æ–ª–æ—Å–∞
                processed = processed + (amount * 1.5)  # –õ—ë–≥–∫–∏–π –ø–æ–¥—ä—ë–º
                
            elif saturation_type == "transistor":
                # –¢—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä–Ω–æ–µ –Ω–∞—Å—ã—â–µ–Ω–∏–µ - –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ
                if processed.max_dBFS > -3:
                    hard_compress = amount * 3
                    processed = processed - hard_compress
            
            self.logger.debug(f"Saturation applied: {saturation_type}, amount {amount}, warmth {warmth}")
            return processed
            
        except Exception as e:
            self.logger.error(f"Saturation processing error: {e}")
            return audio


class StereoProcessor(EffectsProcessor):
    """–°—Ç–µ—Ä–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ"""
    
    def __init__(self):
        super().__init__("Stereo")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """
        –°—Ç–µ—Ä–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
        params: {"width": 0.5-2.0, "imaging": "natural/enhanced/mono"}
        """
        try:
            width = params.get("width", 1.0)
            imaging = params.get("imaging", "natural")
            
            if audio.channels < 2:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç–µ—Ä–µ–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                audio = audio.set_channels(2)
            
            processed = audio
            
            if imaging == "mono":
                # –ú–æ–Ω–æ—Å–∏–≥–Ω–∞–ª
                processed = processed.set_channels(1).set_channels(2)
                
            elif imaging == "enhanced" and width != 1.0:
                if width > 1.0:
                    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å—Ç–µ—Ä–µ–æ
                    # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è - –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∫–∞–Ω–∞–ª–∞–º–∏
                    self.logger.debug(f"Stereo widening: {width:.1f}x")
                    
                elif width < 1.0:
                    # –°—É–∂–µ–Ω–∏–µ —Å—Ç–µ—Ä–µ–æ - –º–∏–∫—Å —Å –º–æ–Ω–æ
                    mono_component = processed.set_channels(1).set_channels(2)
                    mono_gain = (1.0 - width) * -6  # dB 
                    processed = processed.overlay(mono_component + mono_gain)
                    self.logger.debug(f"Stereo narrowing: {width:.1f}x")
            
            return processed
            
        except Exception as e:
            self.logger.error(f"Stereo processing error: {e}")
            return audio


class LimiterProcessor(EffectsProcessor):
    """–õ–∏–º–∏—Ç–µ—Ä - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–ª–∏–ø–ø–∏–Ω–≥–∞"""
    
    def __init__(self):
        super().__init__("Limiter")
    
    async def process(self, audio: AudioSegment, params: Dict) -> AudioSegment:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ª–∏–º–∏—Ç–µ—Ä–∞
        params: {"threshold": db, "ceiling": db, "release": ms}
        """
        try:
            threshold = params.get("threshold", -3)
            ceiling = params.get("ceiling", -0.1)
            
            peak_level = audio.max_dBFS
            
            if peak_level > threshold:
                # –ú—è–≥–∫–æ–µ –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                over_threshold = peak_level - threshold
                if over_threshold > 0:
                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
                    target_peak = min(ceiling, threshold)
                    required_reduction = peak_level - target_peak
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                    processed = audio - required_reduction
                    
                    self.logger.debug(f"Limiting: threshold {threshold}dB, ceiling {ceiling}dB, "
                                    f"reduction {required_reduction:.1f}dB")
                    return processed
            
            return audio
            
        except Exception as e:
            self.logger.error(f"Limiter processing error: {e}")
            return audio


class EffectsChain:
    """–¶–µ–ø–æ—á–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ JSON"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã
        self.processors = {
            "eq": EQProcessor(),
            "compressor": CompressorProcessor(),
            "reverb": ReverbProcessor(),
            "saturation": SaturationProcessor(),
            "stereo": StereoProcessor(),
            "limiter": LimiterProcessor()
        }
        
        self.logger.info(f"‚úÖ Effects chain initialized: {list(self.processors.keys())}")
    
    async def apply_effects(
        self, 
        audio: AudioSegment, 
        effects_config: Dict,
        genre_info: Optional[Dict] = None
    ) -> AudioSegment:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        
        effects_config format:
        {
            "eq": {"low": 2, "mid": 0, "high": 1},
            "compressor": {"ratio": 3.0, "threshold": -10},
            "reverb": {"room_size": 0.3, "wet_level": 0.15}
        }
        """
        if not effects_config:
            return audio
        
        processed = audio
        applied_effects = []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        effect_order = ["eq", "compressor", "saturation", "reverb", "stereo", "limiter"]
        
        for effect_name in effect_order:
            if effect_name in effects_config:
                effect_params = effects_config[effect_name]
                
                if effect_name in self.processors:
                    try:
                        processed = await self.processors[effect_name].process(processed, effect_params)
                        applied_effects.append(effect_name)
                        
                    except Exception as e:
                        self.logger.error(f"Error applying {effect_name}: {e}")
        
        self.logger.info(f"‚ú® Applied effects: {', '.join(applied_effects)}")
        return processed
    
    async def mix_layers(
        self,
        base_layer: bytes,
        stem_layers: Dict[str, bytes], 
        mix_settings: Dict,
        genre_info: Dict
    ) -> AudioSegment:
        """
        –ú–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –¥–æ—Ä–æ–∂–∫–∏ —Å–æ —Å—Ç–µ–º–∞–º–∏
        
        Args:
            base_layer: –û—Å–Ω–æ–≤–Ω–∞—è –¥–æ—Ä–æ–∂–∫–∞ –æ—Ç MusicGen
            stem_layers: –°–ª–æ–≤–∞—Ä—å —Å—Ç–µ–º–æ–≤ {instrument: audio_data}
            mix_settings: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∏–∫—Å–∞ –¥–ª—è –∂–∞–Ω—Ä–∞
            genre_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∂–∞–Ω—Ä–µ
        """
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –±–∞–π—Ç—ã –≤ AudioSegment
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ bytes
            # –í—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞:
            base_audio = AudioSegment.silent(duration=60000)  # 60 —Å–µ–∫—É–Ω–¥
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —É—Ä–æ–≤–µ–Ω—å –±–∞–∑–æ–≤–æ–π –¥–æ—Ä–æ–∂–∫–∏
            base_level = mix_settings.get("base_level", -3)
            mixed = base_audio + base_level
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–µ–º—ã
            stems_level = mix_settings.get("stems_level", -6)
            
            for instrument, stem_data in stem_layers.items():
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º stem_data –≤ AudioSegment
                # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ bytes
                stem_audio = AudioSegment.silent(duration=len(mixed))
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å—Ç–µ–º–∞
                stem_audio = stem_audio + stems_level
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                instrument_settings = self._get_instrument_mix_settings(instrument, genre_info)
                if instrument_settings:
                    stem_audio = await self.apply_effects(stem_audio, instrument_settings)
                
                # –ú–∏–∫—à–∏—Ä—É–µ–º
                mixed = mixed.overlay(stem_audio)
                
                self.logger.debug(f"  üéõÔ∏è Mixed {instrument}: {stems_level:+.1f}dB")
            
            self.logger.info(f"üéöÔ∏è Mixed base + {len(stem_layers)} stems")
            return mixed
            
        except Exception as e:
            self.logger.error(f"‚ùå Mixing error: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –¥–æ—Ä–æ–∂–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return base_audio
    
    def _get_instrument_mix_settings(self, instrument: str, genre_info: Dict) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–∏–∫—Å–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        genre = genre_info.get("name", "")
        
        instrument_effects = {
            "trap": {
                "kick": {"eq": {"low": 3, "mid": 0, "high": -1}},
                "snare": {"eq": {"low": -1, "mid": 2, "high": 4}, "compressor": {"ratio": 2.5}},
                "hihat": {"eq": {"low": -3, "mid": 0, "high": 2}},
                "bass": {"eq": {"low": 2, "mid": -1, "high": -2}, "saturation": {"amount": 0.2}}
            },
            "lofi": {
                "kick": {"eq": {"low": 1, "mid": 0, "high": -3}, "saturation": {"amount": 0.4, "type": "tape"}},
                "snare": {"eq": {"low": 0, "mid": -1, "high": -2}},
                "piano": {"reverb": {"room_size": 0.2, "wet_level": 0.3}, "saturation": {"amount": 0.3, "warmth": 0.5}}
            },
            "house": {
                "kick": {"eq": {"low": 2, "mid": 0, "high": 0}, "compressor": {"ratio": 3.0, "threshold": -8}},
                "bass": {"eq": {"low": 1, "mid": 0, "high": -1}},
                "lead": {"reverb": {"room_size": 0.4, "wet_level": 0.2}}
            },
            "ambient": {
                "pad": {"reverb": {"room_size": 0.8, "wet_level": 0.5, "type": "hall"}},
                "lead": {"reverb": {"room_size": 0.6, "wet_level": 0.4}},
                "fx": {"stereo": {"width": 1.5, "imaging": "enhanced"}}
            }
        }
        
        return instrument_effects.get(genre, {}).get(instrument)
    
    def load_effects_preset(self, preset_path: str) -> Optional[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ—Å–µ—Ç–∞ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(preset_path, 'r', encoding='utf-8') as f:
                preset = json.load(f)
            
            self.logger.info(f"üìã Loaded effects preset: {preset_path}")
            return preset
            
        except Exception as e:
            self.logger.error(f"‚ùå Error loading effects preset {preset_path}: {e}")
            return None
    
    def save_effects_preset(self, effects_config: Dict, preset_path: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ—Å–µ—Ç–∞ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –≤ JSON —Ñ–∞–π–ª"""
        try:
            with open(preset_path, 'w', encoding='utf-8') as f:
                json.dump(effects_config, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üíæ Saved effects preset: {preset_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error saving effects preset {preset_path}: {e}")
            return False


# ===== –£–¢–ò–õ–ò–¢–´ –ò –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò =====

def create_genre_specific_effects(genre: str) -> Dict:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∂–∞–Ω—Ä–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
    effects_templates = {
        "trap": {
            "eq": {"low": 2, "mid": 0, "high": 1},
            "compressor": {"ratio": 2.5, "threshold": -10},
            "saturation": {"amount": 0.15, "type": "transistor"}
        },
        "lofi": {
            "eq": {"low": 1, "mid": -1, "high": -3},
            "saturation": {"amount": 0.4, "type": "tape", "warmth": 0.6},
            "reverb": {"room_size": 0.3, "wet_level": 0.2, "type": "room"}
        },
        "house": {
            "eq": {"low": 1, "mid": 0, "high": 2},
            "compressor": {"ratio": 3.0, "threshold": -8},
            "reverb": {"room_size": 0.4, "wet_level": 0.15, "type": "hall"}
        },
        "ambient": {
            "eq": {"low": 0, "mid": 0, "high": 1},
            "reverb": {"room_size": 0.8, "wet_level": 0.6, "type": "hall"},
            "stereo": {"width": 1.4, "imaging": "enhanced"}
        },
        "dnb": {
            "eq": {"low": 3, "mid": 1, "high": 2},
            "compressor": {"ratio": 4.0, "threshold": -6},
            "saturation": {"amount": 0.2, "type": "transistor"}
        },
        "dubstep": {
            "eq": {"low": 4, "mid": 0, "high": 1},
            "compressor": {"ratio": 5.0, "threshold": -4},
            "saturation": {"amount": 0.3, "type": "transistor"},
            "limiter": {"threshold": -1, "ceiling": -0.1}
        }
    }
    
    return effects_templates.get(genre, {
        "eq": {"low": 0, "mid": 0, "high": 0},
        "compressor": {"ratio": 2.0, "threshold": -12}
    })


def validate_audio_file(filepath: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(filepath):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(filepath)
        if file_size < 1024:  # –ú–µ–Ω—å—à–µ 1KB - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
            return False
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª
        try:
            audio = AudioSegment.from_file(filepath)
            duration = len(audio) / 1000
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            if duration < 0.1 or duration > 600:  # –û—Ç 0.1 —Å–µ–∫ –¥–æ 10 –º–∏–Ω—É—Ç
                return False
                
            return True
            
        except:
            # Fallback —á–µ—Ä–µ–∑ librosa
            y, sr = librosa.load(filepath, duration=1)  # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å–µ–∫—É–Ω–¥—É
            return len(y) > 0
            
    except Exception:
        return False


def extract_bpm_from_filename(filename: str) -> Optional[int]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ BPM –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º"""
    patterns = [
        r'(\d{2,3})\s*bpm',
        r'(\d{2,3})\s*beats',
        r'tempo[\s_-]*(\d{2,3})',
        r'(\d{2,3})[\s_-]*beat',
        r'(\d{2,3})[\s_-]*bpm',
        r'bpm[\s_-]*(\d{2,3})',
        r'(\d{2,3})[\s_-]*tempo'
    ]
    
    filename_lower = filename.lower()
    
    for pattern in patterns:
        match = re.search(pattern, filename_lower)
        if match:
            bpm = int(match.group(1))
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π BPM
            if 60 <= bpm <= 200:
                return bpm
    
    return None


def extract_key_from_filename(filename: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
    patterns = [
        r'\b([a-g][#b]?)\s*m(?:inor|in)?\b',
        r'\b([a-g][#b]?)\s*maj(?:or)?\b',
        r'\b([a-g][#b]?)[\s_-]*(?:key|scale)\b',
        r'key[\s_-]*([a-g][#b]?)\b'
    ]
    
    filename_lower = filename.lower()
    
    for pattern in patterns:
        match = re.search(pattern, filename_lower)
        if match:
            key = match.group(1).upper()
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è
            key = key.replace('B', 'b')  # –ë–µ–º–æ–ª—å
            return key
    
    return None


async def batch_process_samples(
    sample_paths: List[str], 
    processor_func, 
    max_workers: int = 4,
    batch_size: int = 50
) -> List[Any]:
    """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ç–º–ø–ª–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–µ—Å—É—Ä—Å–æ–≤"""
    results = []
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
    for i in range(0, len(sample_paths), batch_size):
        batch = sample_paths[i:i + batch_size]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            batch_results = await asyncio.get_event_loop().run_in_executor(
                executor,
                lambda: [processor_func(path) for path in batch]
            )
            
        results.extend(batch_results)
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
        await asyncio.sleep(0.1)
    
    return results


class SampleQualityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ —Å—ç–º–ø–ª–æ–≤"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.QualityAnalyzer")
    
    def analyze_sample_quality(self, filepath: str, audio_analysis: Dict) -> Dict:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—ç–º–ø–ª–∞"""
        quality_metrics = {
            "overall_score": 0.0,
            "audio_quality": 0.0,
            "metadata_completeness": 0.0,
            "spectral_quality": 0.0,
            "dynamic_range": 0.0,
            "issues": []
        }
        
        try:
            # –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ –∫–∞—á–µ—Å—Ç–≤–∞
            audio_quality = self._analyze_audio_quality(audio_analysis)
            quality_metrics["audio_quality"] = audio_quality["score"]
            quality_metrics["issues"].extend(audio_quality["issues"])
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Ç—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata_quality = self._analyze_metadata_completeness(filepath, audio_analysis)
            quality_metrics["metadata_completeness"] = metadata_quality["score"]
            
            # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            spectral_quality = self._analyze_spectral_quality(audio_analysis)
            quality_metrics["spectral_quality"] = spectral_quality["score"]
            quality_metrics["issues"].extend(spectral_quality["issues"])
            
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
            dynamic_range = self._analyze_dynamic_range(audio_analysis)
            quality_metrics["dynamic_range"] = dynamic_range["score"]
            quality_metrics["issues"].extend(dynamic_range["issues"])
            
            # –û–±—â–∏–π —Å–∫–æ—Ä (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)
            weights = [0.4, 0.2, 0.2, 0.2]  # –∞—É–¥–∏–æ, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —Å–ø–µ–∫—Ç—Ä, –¥–∏–Ω–∞–º–∏–∫–∞
            scores = [
                quality_metrics["audio_quality"],
                quality_metrics["metadata_completeness"],
                quality_metrics["spectral_quality"],
                quality_metrics["dynamic_range"]
            ]
            
            quality_metrics["overall_score"] = sum(w * s for w, s in zip(weights, scores))
            
        except Exception as e:
            self.logger.error(f"Quality analysis error for {filepath}: {e}")
            quality_metrics["issues"].append(f"Analysis error: {e}")
        
        return quality_metrics
    
    def _analyze_audio_quality(self, audio_analysis: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª–∞"""
        score = 1.0
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Ä–æ–≤–Ω—è —ç–Ω–µ—Ä–≥–∏–∏
        energy = audio_analysis.get("energy", 0.5)
        if energy < 0.05:
            score -= 0.3
            issues.append("Very low energy level")
        elif energy > 0.95:
            score -= 0.2
            issues.append("Possible clipping detected")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        spectral_centroid = audio_analysis.get("spectral_centroid", 0)
        if spectral_centroid == 0:
            score -= 0.4
            issues.append("No spectral content detected")
        elif spectral_centroid < 100:
            score -= 0.2
            issues.append("Very low spectral content")
        
        return {"score": max(0.0, score), "issues": issues}
    
    def _analyze_metadata_completeness(self, filepath: str, audio_analysis: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Ç—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        score = 0.0
        
        # –ë–æ–Ω—É—Å –∑–∞ BPM –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if extract_bpm_from_filename(os.path.basename(filepath)):
            score += 0.3
        if audio_analysis.get("tempo", 120) != 120:  # –ù–µ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            score += 0.2
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        if extract_key_from_filename(os.path.basename(filepath)):
            score += 0.2
        if audio_analysis.get("key"):
            score += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        filename = os.path.basename(filepath).lower()
        if len([word for word in filename.split() if len(word) > 2]) >= 2:
            score += 0.2
        
        return {"score": min(1.0, score)}
    
    def _analyze_spectral_quality(self, audio_analysis: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"""
        score = 1.0
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        spectral_rolloff = audio_analysis.get("spectral_rolloff", 0)
        spectral_centroid = audio_analysis.get("spectral_centroid", 0)
        
        if spectral_rolloff == 0 or spectral_centroid == 0:
            score -= 0.5
            issues.append("Missing spectral analysis data")
        else:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å–ø–µ–∫—Ç—Ä–∞
            if spectral_centroid > spectral_rolloff * 0.8:
                score -= 0.2
                issues.append("Unbalanced spectral distribution")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ zero crossing rate
        zcr = audio_analysis.get("zero_crossing_rate", 0)
        if zcr > 0.3:
            score -= 0.1
            issues.append("High zero crossing rate (possible noise)")
        
        return {"score": max(0.0, score), "issues": issues}
    
    def _analyze_dynamic_range(self, audio_analysis: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞"""
        score = 1.0
        issues = []
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–µ—Ä–≥–∏–∏ –∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        energy = audio_analysis.get("energy", 0.5)
        rhythmic_complexity = audio_analysis.get("rhythmic_complexity", 0)
        
        # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Å–∂–∞—Ç—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
        if energy > 0.9:
            score -= 0.3
            issues.append("Possibly over-compressed (limited dynamic range)")
        
        # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤ —ç–Ω–µ—Ä–≥–∏—á–Ω–æ–º —Ç—Ä–µ–∫–µ
        if energy > 0.6 and rhythmic_complexity < 1.0:
            score -= 0.1
            issues.append("Low rhythmic complexity for energetic content")
        
        return {"score": max(0.0, score), "issues": issues}


# ===== –≠–ö–°–ü–û–†–¢ –û–°–ù–û–í–ù–´–• –ö–õ–ê–°–°–û–í =====
__all__ = [
    'SemanticSampleEngine',
    'EffectsChain', 
    'SampleMetadata',
    'SampleQualityAnalyzer',
    'create_genre_specific_effects',
    'validate_audio_file',
    'extract_bpm_from_filename',
    'extract_key_from_filename',
    'batch_process_samples'
]