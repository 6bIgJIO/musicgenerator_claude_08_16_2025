# semantic_engine

import logging
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer, util

try:
    from sentence_transformers import SentenceTransformer, util
    SBERT_AVAILABLE = True
    logging.info("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ (SBERT)...")
    sbert_model = SentenceTransformer("all-MiniLM-L6-v2")
except ImportError as e:
    SBERT_AVAILABLE = False
    logging.warning(f"‚ö†Ô∏è SBERT –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    sbert_model = None

def embed_texts(texts: List[str]):
    """–≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤"""
    if not SBERT_AVAILABLE or sbert_model is None:
        logging.warning("‚ö†Ô∏è SBERT –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
        return None
    
    try:
        return sbert_model.encode(texts, convert_to_tensor=True)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        return None

def semantic_search(query: str, candidates: List[str], top_k: int = 5) -> List[Tuple[str, float]]:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
    if not SBERT_AVAILABLE or sbert_model is None:
        # Fallback: –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞–º
        logging.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –≤–º–µ—Å—Ç–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ")
        matches = []
        query_lower = query.lower()
        
        for candidate in candidates:
            candidate_lower = candidate.lower()
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Å–ª–æ–≤
            query_words = set(query_lower.split())
            candidate_words = set(candidate_lower.split())
            score = len(query_words.intersection(candidate_words)) / max(len(query_words), 1)
            
            if score > 0 or query_lower in candidate_lower:
                matches.append((candidate, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º top_k
        matches.sort(key=lambda x: x[1], reverse=True)
        return matches[:top_k]
    
    try:
        logging.info(f"üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è: {query}")
        query_emb = embed_texts([query])
        if query_emb is None:
            return []
        
        cand_emb = embed_texts(candidates)
        if cand_emb is None:
            return []
        
        hits = util.semantic_search(query_emb, cand_emb, top_k=top_k)[0]
        return [(candidates[hit['corpus_id']], hit['score']) for hit in hits]
    
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
        return []

def select_samples_by_semantics(prompt: str, sample_index: Dict[str, Dict], max_results: int = 10) -> List[str]:
    """–í—ã–±–æ—Ä —Å—ç–º–ø–ª–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ"""
    if not sample_index:
        logging.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å —Å—ç–º–ø–ª–æ–≤")
        return []
    
    try:
        candidates = list(sample_index.keys())
        if not candidates:
            return []
        
        ranked = semantic_search(prompt, candidates, top_k=max_results)
        result = [name for name, score in ranked if score > 0.1]  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        
        logging.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(result)}")
        return result[:max_results]
    
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ —Å—ç–º–ø–ª–æ–≤: {e}")
        return []
